{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d97f7c5d-46f3-4cbd-80ad-f1e50cd65096",
   "metadata": {},
   "source": [
    "# Deep Learning Applications: Laboratory #1\n",
    "\n",
    "In this first laboratory we will work relatively simple architectures to get a feel for working with Deep Models. This notebook is designed to work with PyTorch, but as I said in the introductory lecture: please feel free to use and experiment with whatever tools you like.\n",
    "\n",
    "**Important Notes**:\n",
    "1. Be sure to **document** all of your decisions, as well as your intermediate and final results. Make sure your conclusions and analyses are clearly presented. Don't make us dig into your code or walls of printed results to try to draw conclusions from your code.\n",
    "2. If you use code from someone else (e.g. Github, Stack Overflow, ChatGPT, etc) you **must be transparent about it**. Document your sources and explain how you adapted any partial solutions to creat **your** solution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ed8906-bd19-4b4f-8b79-4feae355ffd6",
   "metadata": {},
   "source": [
    "## Exercise 1: Warming Up\n",
    "In this series of exercises I want you to try to duplicate (on a small scale) the results of the ResNet paper:\n",
    "\n",
    "> [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385), Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR 2016.\n",
    "\n",
    "We will do this in steps using a Multilayer Perceptron on MNIST.\n",
    "\n",
    "Recall that the main message of the ResNet paper is that **deeper** networks do not **guarantee** more reduction in training loss (or in validation accuracy). Below you will incrementally build a sequence of experiments to verify this for an MLP. A few guidelines:\n",
    "\n",
    "+ I have provided some **starter** code at the beginning. **NONE** of this code should survive in your solutions. Not only is it **very** badly written, it is also written in my functional style that also obfuscates what it's doing (in part to **discourage** your reuse!). It's just to get you *started*.\n",
    "+ These exercises ask you to compare **multiple** training runs, so it is **really** important that you factor this into your **pipeline**. Using [Tensorboard](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html) is a **very** good idea -- or, even better [Weights and Biases](https://wandb.ai/site).\n",
    "+ You may work and submit your solutions in **groups of at most two**. Share your ideas with everyone, but the solutions you submit *must be your own*.\n",
    "\n",
    "First some boilerplate to get you started, then on to the actual exercises!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb2b6d1-3df0-464c-9a5f-8c611257a971",
   "metadata": {},
   "source": [
    "### Preface: Some code to get you started\n",
    "\n",
    "What follows is some **very simple** code for training an MLP on MNIST. The point of this code is to get you up and running (and to verify that your Python environment has all needed dependencies).\n",
    "\n",
    "**Note**: As you read through my code and execute it, this would be a good time to think about *abstracting* **your** model definition, and training and evaluation pipelines in order to make it easier to compare performance of different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3a8282-2322-4dca-b76e-2f3863bc75fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start with some standard imports.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import Subset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cc12cc-8422-47bf-8d8e-0950ac05ae96",
   "metadata": {},
   "source": [
    "#### Data preparation\n",
    "\n",
    "Here is some basic dataset loading, validation splitting code to get you started working with MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272a69db-0416-444a-9be4-5f055ff48bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard MNIST transform.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load MNIST train and test.\n",
    "ds_train = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "ds_test = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split train into train and validation.\n",
    "val_size = 5000\n",
    "I = np.random.permutation(len(ds_train))\n",
    "ds_val = Subset(ds_train, I[:val_size])\n",
    "ds_train = Subset(ds_train, I[val_size:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e05e96-7707-4490-98b8-50cb5e330af1",
   "metadata": {},
   "source": [
    "#### Boilerplate training and evaluation code\n",
    "\n",
    "This is some **very** rough training, evaluation, and plotting code. Again, just to get you started. I will be *very* disappointed if any of this code makes it into your final submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcce348-f603-4d57-b9a8-5b1c6eba28ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Function to train a model for a single epoch over the data loader.\n",
    "def train_epoch(model, dl, opt, epoch='Unknown', device='cpu'):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for (xs, ys) in tqdm(dl, desc=f'Training epoch {epoch}', leave=True):\n",
    "        xs = xs.to(device)\n",
    "        ys = ys.to(device)\n",
    "        opt.zero_grad()\n",
    "        logits = model(xs)\n",
    "        loss = F.cross_entropy(logits, ys)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "    return np.mean(losses)\n",
    "\n",
    "# Function to evaluate model over all samples in the data loader.\n",
    "def evaluate_model(model, dl, device='cpu'):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    gts = []\n",
    "    for (xs, ys) in tqdm(dl, desc='Evaluating', leave=False):\n",
    "        xs = xs.to(device)\n",
    "        preds = torch.argmax(model(xs), dim=1)\n",
    "        gts.append(ys)\n",
    "        predictions.append(preds.detach().cpu().numpy())\n",
    "        \n",
    "    # Return accuracy score and classification report.\n",
    "    return (accuracy_score(np.hstack(gts), np.hstack(predictions)),\n",
    "            classification_report(np.hstack(gts), np.hstack(predictions), zero_division=0, digits=3))\n",
    "\n",
    "# Simple function to plot the loss curve and validation accuracy.\n",
    "def plot_validation_curves(losses_and_accs):\n",
    "    losses = [x for (x, _) in losses_and_accs]\n",
    "    accs = [x for (_, x) in losses_and_accs]\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Average Training Loss per Epoch')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(accs)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.title(f'Best Accuracy = {np.max(accs)} @ epoch {np.argmax(accs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875008c3-306c-4e39-a845-d7bda7862621",
   "metadata": {},
   "source": [
    "#### A basic, parameterized MLP\n",
    "\n",
    "This is a very basic implementation of a Multilayer Perceptron. Don't waste too much time trying to figure out how it works -- the important detail is that it allows you to pass in a list of input, hidden layer, and output *widths*. **Your** implementation should also support this for the exercises to come."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1e503a-37df-4fb9-94e7-85d0adb494bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(nin, nout) for (nin, nout) in zip(layer_sizes[:-1], layer_sizes[1:])])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return reduce(lambda f, g: lambda x: g(F.relu(f(x))), self.layers, lambda x: x.flatten(1))(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2cad13-ee2c-4e43-b5c7-31760da8c2df",
   "metadata": {},
   "source": [
    "### Exercise 1.1: A baseline MLP\n",
    "\n",
    "Implement a *simple* Multilayer Perceptron to classify the 10 digits of MNIST (e.g. two *narrow* layers). Use my code above as inspiration, but implement your own training pipeline -- you will need it later. Train this model to convergence, monitoring (at least) the loss and accuracy on the training and validation sets for every epoch. Below I include a basic implementation to get you started -- remember that you should write your *own* pipeline!\n",
    "\n",
    "**Note**: This would be a good time to think about *abstracting* your model definition, and training and evaluation pipelines in order to make it easier to compare performance of different models.\n",
    "\n",
    "**Important**: Given the *many* runs you will need to do, and the need to *compare* performance between them, this would **also** be a great point to study how **Tensorboard** or **Weights and Biases** can be used for performance monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd5a6380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with some standard imports.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import random\n",
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import Subset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import tensorboard\n",
    "import os\n",
    "import csv\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
    "import json\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "046b9751",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed= 123\n",
    "random.seed(seed)             \n",
    "np.random.seed(seed)          \n",
    "torch.manual_seed(seed)       \n",
    "torch.cuda.manual_seed(seed)  \n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ed78751",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(SkipBlock,self).__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, out_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(out_dim, out_dim)\n",
    "        if in_dim != out_dim:\n",
    "            self.projection = nn.Linear(in_dim, out_dim)\n",
    "        else:\n",
    "            self.projection = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.projection(x)\n",
    "        out = self.relu(self.fc1(x))\n",
    "        out = self.fc2(out)\n",
    "        return self.relu(out + identity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19d96405-36e7-4074-803c-fb02576cd528",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class My_MLP(nn.Module):\n",
    "    def __init__(self,layer_sizes,use_skip=False):\n",
    "        super(My_MLP, self).__init__()\n",
    "        layers=[]\n",
    "        layers.append(nn.Flatten()) \n",
    "        for in_dim, out_dim in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            if use_skip==True:\n",
    "                layers.append(SkipBlock(in_dim, out_dim))\n",
    "            else:\n",
    "                layers.append(nn.Linear(in_dim, out_dim))\n",
    "                if out_dim != layer_sizes[-1]:  \n",
    "                    layers.append(nn.ReLU())\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a11a4efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Data():\n",
    "    transform= transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    ds_train= MNIST(root='./data', train= True, download=True, transform=transform)\n",
    "    ds_test=MNIST(root='./data', train= False, download=True, transform=transform)\n",
    "\n",
    "    \n",
    "    return ds_train, ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ee47734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Validation_Model(model, dl_val,device, batch_size):\n",
    "    model.eval()\n",
    "    predictions=[]\n",
    "    ground_truth=[]\n",
    "    criterion= torch.nn.CrossEntropyLoss()\n",
    "    losses=[]\n",
    "    dl_validation= DataLoader(dl_val, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    for (data, labels) in tqdm(dl_validation, desc=\"Evaluating\", leave=False):\n",
    "        data= data.to(device)\n",
    "        labels= labels.to(device)\n",
    "        logits= model(data)\n",
    "        loss= criterion(logits, labels)\n",
    "        prediction= torch.argmax(logits, dim=1)\n",
    "        losses.append(loss.item())\n",
    "        ground_truth.append(labels.detach().cpu().numpy())\n",
    "        predictions.append(prediction.detach().cpu().numpy())\n",
    "    return (accuracy_score(np.hstack(ground_truth), np.hstack(predictions)),\n",
    "            classification_report(np.hstack(ground_truth), np.hstack(predictions), zero_division=0, digits=3),\n",
    "            np.mean(losses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c00f5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_norms(model):\n",
    "    grad_norms = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norms[name] = param.grad.norm().item()\n",
    "    return grad_norms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec424c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training_Model(model,X, file_writer,device,optimizer=None,epochs=50,batch_size=8, learning_rate=0.001, weight_decay=0.001,study_grad=False):\n",
    "    total_size = len(X)\n",
    "    val_size = int(0.2 * total_size)  \n",
    "    train_size = total_size - val_size\n",
    "\n",
    "    indices = np.random.permutation(total_size)\n",
    "    val_indices = indices[:val_size]\n",
    "    train_indices = indices[val_size:]\n",
    "\n",
    "    ds_train = Subset(X, train_indices)\n",
    "    ds_val = Subset(X, val_indices)\n",
    "    dl_train=DataLoader(ds_train, batch_size=batch_size,shuffle=True )\n",
    "    \n",
    "    if optimizer is None:\n",
    "        optimizer= torch.optim.Adam(params=model.parameters(), lr= learning_rate, weight_decay=weight_decay)\n",
    "    criterion= torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Model Training\"):\n",
    "        model.train()\n",
    "        losses=[]\n",
    "        count=0\n",
    "        for (data,labels) in tqdm(dl_train, desc=f'Training epoch {epoch}', leave=True):\n",
    "            data= data.to(device)\n",
    "            labels= labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output= model(data)\n",
    "            loss= criterion(output, labels)\n",
    "            loss.backward()\n",
    "            if count == 0 and study_grad==True:\n",
    "                grad_norms = get_grad_norms(model)\n",
    "\n",
    "                # Esempio: loggare su TensorBoard\n",
    "                for name, norm in grad_norms.items():\n",
    "                    file_writer.add_scalar(f\"GradNorms/{name}\", norm, epoch)\n",
    "            count=1\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        loss_average= np.mean(losses)\n",
    "        \n",
    "        \n",
    "        accurancy, report_dict, losses_val= Validation_Model(model, ds_val, device, batch_size)\n",
    "        print(f\"Training Loss: {loss_average} of Epoch {epoch}\")\n",
    "        print(f\"Validation Loss: {losses_val}\")\n",
    "        file_writer.add_scalars(\n",
    "                \"Loss\",\n",
    "                {\n",
    "                    \"Train\": loss_average,\n",
    "                    \"Validation\": losses_val\n",
    "                },\n",
    "                epoch\n",
    "            )\n",
    "        file_writer.add_scalar(\"Train/Accurancy\", accurancy, epoch)\n",
    "        report_str = json.dumps(report_dict, indent=4)\n",
    "        file_writer.add_text(\"Train/Classification Report\", f\"<pre>{report_str}</pre>\", epoch)\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f529e44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in questo metodo dovro usare la CNN come estrattore di features è usare una libreiria di scikit\n",
    "# Devo dunque richiamare i layers fino al pooling togliendo il fc\n",
    "class Feature_Extractor(nn.Module):\n",
    "    def __init__(self,model):\n",
    "        super(Feature_Extractor,self).__init__()\n",
    "        #In questo modo tolgo l'ultimop fc mantenendo le attivazioni del pooling\n",
    "        self.backbone= nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= self.backbone(x)\n",
    "        return torch.flatten(x,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cd8d11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_extractor(datloader, model,device,file_writer):\n",
    "    features=[]\n",
    "    labels= []\n",
    "    with torch.no_grad():\n",
    "        for data, label in datloader:\n",
    "            data= data.to(device)\n",
    "\n",
    "            feature= model(data)\n",
    "            features.append(feature.cpu().numpy())\n",
    "            labels.append(label)\n",
    "\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    return features,labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98dca72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_classifier(model,train_loader, test_loader, device, file_writer,type_of_classifier=\"svm\"):\n",
    "    model= Feature_Extractor(model)\n",
    "    model= model.to(device)\n",
    "    features_train, labels_train=features_extractor(train_loader, model,device, file_writer)\n",
    "    features_test, labels_test=features_extractor(test_loader, model,device, file_writer)\n",
    "\n",
    "    if type_of_classifier==\"svm\":\n",
    "        clf= LinearSVC(max_iter=2000)\n",
    "    elif type_of_classifier==\"knn\":\n",
    "        clf = KNeighborsClassifier(n_neighbors=5)\n",
    "    else:\n",
    "        clf= GaussianNB()\n",
    "    clf.fit(features_train,labels_train)\n",
    "    acc= clf.score(features_test,labels_test) *100\n",
    "    file_writer.add_scalar(\"Accurancy\", acc, 0)\n",
    "    y_pred = clf.predict(features_test)\n",
    "    acc = accuracy_score(labels_test, y_pred)\n",
    "    precision = precision_score(labels_test, y_pred, average=\"macro\")\n",
    "    recall = recall_score(labels_test, y_pred, average=\"macro\")\n",
    "    f1 = f1_score(labels_test, y_pred, average=\"macro\")\n",
    "\n",
    "    file_writer.add_scalar(\"Accuracy\", acc * 100, 0)\n",
    "    file_writer.add_scalar(\"Precision_macro\", precision * 100, 0)\n",
    "    file_writer.add_scalar(\"Recall_macro\", recall * 100, 0)\n",
    "    file_writer.add_scalar(\"F1_macro\", f1 * 100, 0)\n",
    "\n",
    "\n",
    "    return acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0ff063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dunque scongelo gli ultimi due layer e ovviamente anche il fully connected\n",
    "#pooling non serve essendo che compie operazioni matematiche quindi è un layer che non contiene i pesi e non allenabile\n",
    "def fine_tuning(model, device, num_classes, optim, learning_rate,weight_decay, momentum, block_unfreeze):\n",
    "    in_features = model.fully_connected.in_features\n",
    "    model.fully_connected = nn.Linear(in_features, num_classes)\n",
    "    model = model.to(device)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for name, param in model.named_parameters():\n",
    "        if any(b in name for b in block_unfreeze):\n",
    "            param.requires_grad = True\n",
    "    for name, module in model.named_modules():\n",
    "        if any(b in name for b in block_unfreeze):\n",
    "            if isinstance(module, nn.BatchNorm2d):\n",
    "                module.train()  \n",
    "        else:\n",
    "            if isinstance(module, nn.BatchNorm2d):\n",
    "                module.eval()   # \n",
    "\n",
    "    \n",
    "    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\n",
    "    # Crea l'ottimizzatore\n",
    "    if optim.lower() == \"adam\":\n",
    "        optimizer = torch.optim.Adam(trainable_params, lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optim.lower() == \"adamw\":\n",
    "        optimizer = torch.optim.AdamW(trainable_params, lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optim.lower() == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(trainable_params, lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    else:  # RMSprop\n",
    "        optimizer = torch.optim.RMSprop(trainable_params, lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "    return model, optimizer\n",
    "\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "724d0c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Per fare fine-tuning devo ovviamente cambiare qualcosa in questo caso richiesto di prendere il modello e fare feature extractor e cambiare anche qualche cosa come l'ottimizzatore\n",
    "#passo 1 con uno nuovo modello e usare la rete come features extractor \n",
    "#passo 2 devo scongelare qualche layer e da li riaddestrarli con un nuovo classificatore \n",
    "#quindi vorrei fare sia l'esempio con SVN-KNN o altro\n",
    "#vorrei provare quindia nche ADAMW SDG per completare \n",
    "# nello scongelare i layer dobbiamo prendere quelli più profondi perchè i primi servono a estrarre cartteristiche delle immagini\n",
    "#quindi anhce per cifar100 essendo immagini molto simili non c'è bisogno di riaddestrare\n",
    "# invece i layer più profondi vanno a generare features più specifiche per il tipo di dataset preso.\n",
    "def Customize_model(model, X_train, X_test, file_writer,num_classes, device, \n",
    "                    lr, weight_decay, batch_size,freeze_layers,cl,optim, momentum,\n",
    "                    block_unfreeze):\n",
    "    \n",
    "    train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=False) \n",
    "    test_loader  = DataLoader(X_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    if freeze_layers==False:\n",
    "        acc_cl=custom_classifier(model, train_loader,test_loader, device, file_writer,cl )\n",
    "        print(f'Accurancy of classifier {cl}: {acc_cl} /n')\n",
    "        return acc_cl, \"classifier\"\n",
    "    \n",
    "    else:\n",
    "        model, optimizer=fine_tuning(model,device, num_classes,optim,lr, weight_decay,momentum,block_unfreeze)\n",
    "        return (model,optimizer), \"fine_tuning\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "445cb699",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "    def __init__(self,model,\n",
    "                 logdir,\n",
    "                 date, \n",
    "                 num_classes,\n",
    "                 depth=None,\n",
    "                 epochs=50, \n",
    "                 batch_size=8, \n",
    "                 learning_rate=0.001, \n",
    "                 weight_decay=0.001,\n",
    "                 path_exp= \"Simple_MLP\",\n",
    "                 study_grad=False,\n",
    "                 freeze_layers=False,\n",
    "                 classificator= None,\n",
    "                 optimizer= None,\n",
    "                 momentum=None,\n",
    "                 block_to_unfreeze= None\n",
    "                 ):\n",
    "        super(Trainer,self).__init__()\n",
    "        self.model=model\n",
    "        self.study_grad=study_grad\n",
    "        self.epochs=epochs\n",
    "        self.batch_size=batch_size\n",
    "        self.learning_rate= learning_rate\n",
    "        self.weight_decay= weight_decay\n",
    "        self.best_model= None\n",
    "        self.device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.path_experiments=f'{path_exp}/Run_{date}'\n",
    "        self.num_classes= num_classes\n",
    "        self.file_writer= SummaryWriter(logdir)\n",
    "        self.depth=depth\n",
    "        self.optimizer=optimizer\n",
    "        self.freeze_layers= freeze_layers\n",
    "        self.classificator=classificator\n",
    "        self.momentum= momentum\n",
    "        self.block_unfreeze= block_to_unfreeze\n",
    "\n",
    "\n",
    "    def get_hyperparamtres_dict(self):\n",
    "        result={\n",
    "            'Epochs': self.epochs,\n",
    "            'Batch size':self.batch_size,\n",
    "            'Learning Rate': self.learning_rate,\n",
    "            'Weight Decay':self.weight_decay,\n",
    "            'Num Classes': self.num_classes,\n",
    "            'Freeze_layers': self.freeze_layers\n",
    "            \n",
    "        }\n",
    "        if self.depth is not None:\n",
    "            result[\"Depth\"]= self.depth\n",
    "        if self.momentum is not None:\n",
    "            result[\"Momentum\"]= self.momentum\n",
    "        if self.classificator is not None:\n",
    "            result[\"Classificator\"]= self.classificator\n",
    "        if self.optimizer is not None:\n",
    "            result[\"Optimizer\"]= self.optimizer\n",
    "        if self.block_unfreeze is not None:\n",
    "            result[\"Block_Unfreeze\"]= self.block_unfreeze\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def save_hyperparametres(self):\n",
    "        hyperparametres_dict= self.get_hyperparamtres_dict()\n",
    "        path= os.path.join(self.path_experiments, 'hyperparametres.csv')\n",
    "        file_exists = os.path.isfile(path)\n",
    "        is_empty = not file_exists or os.stat(path).st_size == 0\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        with open(path, mode='a', newline='') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=hyperparametres_dict.keys())\n",
    "            if is_empty:\n",
    "                writer.writeheader()\n",
    "            writer.writerow(hyperparametres_dict)\n",
    "\n",
    "    def Train(self,X):\n",
    "        if not os.path.exists(self.path_experiments):\n",
    "            os.makedirs(self.path_experiments)\n",
    "        self.save_hyperparametres()\n",
    "        self.model.to(self.device)\n",
    "        self.best_model= Training_Model(self.model,X, self.file_writer,self.device, self.optimizer,self.epochs,self.batch_size, self.learning_rate, self.weight_decay)\n",
    "        torch.save(self.best_model.state_dict(), os.path.join(self.path_experiments,'best_model.pt'))\n",
    "\n",
    "    def Fine_Tuning(self,X_train,X_test):\n",
    "        result, type=Customize_model(self.model, \n",
    "                                     X_train, X_test, \n",
    "                                     self.file_writer,\n",
    "                                     self.num_classes, \n",
    "                                     self.device,\n",
    "                                     self.learning_rate,\n",
    "                                     self.weight_decay, \n",
    "                                     self.batch_size,\n",
    "                                     self.freeze_layers,\n",
    "                                     self.classificator,\n",
    "                                     self.optimizer, \n",
    "                                     self.momentum,\n",
    "                                     self.block_unfreeze)\n",
    "        \n",
    "        if type==\"fine_tuning\":\n",
    "            self.model, self.optimizer= result\n",
    "            self.Train(X_train)\n",
    "            self.Test(X_test)\n",
    "\n",
    "    def Test(self,X, model=None):\n",
    "        if model is None:\n",
    "            acc, report_dict, loss= Validation_Model(self.best_model, X, self.device,self.batch_size)\n",
    "            self.file_writer.add_scalar(\"Test/Accurancy\", acc, 0)\n",
    "            self.file_writer.add_text(\"Test/Classification Report\", f\"<pre>{report_dict}</pre>\", 1)\n",
    "            self.file_writer.close()\n",
    "        else:\n",
    "            acc, report_dict, loss= Validation_Model(model, X, self.device,self.batch_size)\n",
    "            self.file_writer.add_scalar(\"Test/Accurancy\", acc, 0)\n",
    "            self.file_writer.add_text(\"Test/Classification Report\", f\"<pre>{report_dict}</pre>\", 1)\n",
    "            self.file_writer.close()\n",
    "            \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32a9ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "now= datetime.datetime.now()\n",
    "data_ora_formattata = now.strftime(\"%d_%m_%yT%H_%M\")\n",
    "name= f'run_{data_ora_formattata}'\n",
    "logdir= f'tensorboard/Sample_MLP/{name}'\n",
    "print(f\"Train Model Sample MLP on MNIST\")\n",
    "input_size = 28*28\n",
    "width = 16\n",
    "depth = 2\n",
    "channels= [input_size] + [width]*depth + [10]\n",
    "\n",
    "minist_train, minist_test= Load_Data()\n",
    "model= My_MLP(channels)\n",
    "trainer= Trainer(model, logdir,data_ora_formattata,minist_train.classes,channels,100,128,0.001) \n",
    "\n",
    "trainer.Train(minist_train)\n",
    "trainer.Test(minist_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8ad9b-e3ae-4c49-9bec-35aaea149b08",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Adding Residual Connections\n",
    "\n",
    "Implement a variant of your parameterized MLP network to support **residual** connections. Your network should be defined as a composition of **residual MLP** blocks that have one or more linear layers and add a skip connection from the block input to the output of the final linear layer.\n",
    "\n",
    "**Compare** the performance (in training/validation loss and test accuracy) of your MLP and ResidualMLP for a range of depths. Verify that deeper networks **with** residual connections are easier to train than a network of the same depth **without** residual connections.\n",
    "\n",
    "**For extra style points**: See if you can explain by analyzing the gradient magnitudes on a single training batch *why* this is the case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bcff82-756a-4ffa-92ae-a939fa21f5fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "now= datetime.datetime.now()\n",
    "data_ora_formattata = now.strftime(\"%d_%m_%yT%H_%M\")\n",
    "name= f'run_{data_ora_formattata}'\n",
    "print(\"Training Residual Net vs Simple MLP\")\n",
    "input_size = 28*28\n",
    "width = 16\n",
    "depths = [2,6,10]\n",
    "\n",
    "minist_train, minist_test= Load_Data()\n",
    "\n",
    "for depth in depths:\n",
    "    for use_skip in [True,False]:\n",
    "        channels= [input_size] + [width]*depth + [10]\n",
    "        model= My_MLP(channels,use_skip=use_skip )\n",
    "        if use_skip:\n",
    "            print(f'Run Training of Residual_depth{depth} ')\n",
    "            logdir= f'tensorboard/Residual_vs_Simple_MLP/{name}/Residual_depth{depth}'\n",
    "            path=f\"Residual_vs_Simple_MLP/Residual_depth{depth}\"\n",
    "            \n",
    "        else:\n",
    "            print(f'Run Training of Simple_depth{depth} ')\n",
    "            logdir= f'tensorboard/Residual_vs_Simple_MLP/{name}/Simple_depth{depth}'\n",
    "            path=f\"Residual_vs_Simple_MLP/Simple_depth{depth}\"\n",
    "            \n",
    "        trainer= Trainer(model,logdir,data_ora_formattata,minist_train.classes,0,100,128,0.001,0.001,path,True)\n",
    "\n",
    "        trainer.Train(minist_train)\n",
    "        trainer.Test(minist_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887db849",
   "metadata": {},
   "source": [
    "Classic MLP (without skip connections):\n",
    "In the gradient plots, a typical phenomenon of deep networks can be observed: the gradient norms decrease rapidly in the deeper layers, close to the input. This is caused by the vanishing gradient problem, where the error signal backpropagating through the network becomes progressively attenuated. As a result, the early layers receive very small gradients, update their weights slowly, and contribute little to learning. This explains why a classic MLP takes longer to converge and may get stuck at relatively low performance.    \n",
    "\n",
    "MLP with skip connections:\n",
    "Adding bypass connections between layers allows gradients to skip certain layers and reach deeper layers more easily. In the gradnorm plots, it can be observed that the gradients remain more balanced across all layers, including the deeper ones. This enables the early layers to update more effectively, improving training stability and accelerating convergence. In fact, skip connections mitigate the vanishing gradient problem, making the learning of deep layers more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c59bdd8-3377-4311-b45f-511c2fb0b53e",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Rinse and Repeat (but with a CNN)\n",
    "\n",
    "Repeat the verification you did above, but with **Convolutional** Neural Networks. If you were careful about abstracting your model and training code, this should be a simple exercise. Show that **deeper** CNNs *without* residual connections do not always work better and **even deeper** ones *with* residual connections.\n",
    "\n",
    "**Hint**: You probably should do this exercise using CIFAR-10, since MNIST is *very* easy (at least up to about 99% accuracy).\n",
    "\n",
    "**Tip**: Feel free to reuse the ResNet building blocks defined in `torchvision.models.resnet` (e.g. [BasicBlock](https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L59) which handles the cascade of 3x3 convolutions, skip connections, and optional downsampling). This is an excellent exercise in code diving. \n",
    "\n",
    "**Spoiler**: Depending on the optional exercises you plan to do below, you should think *very* carefully about the architectures of your CNNs here (so you can reuse them!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "469e81a3-08ca-4549-a2f8-f47cf5a0308b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.resnet import BasicBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e06d1565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_data_Cifar10():\n",
    "    transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "    \n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87232dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_Block_CNN(nn.Module):\n",
    "    def __init__(self, in_channels, use_resnet):\n",
    "        super(Residual_Block_CNN,self).__init__()\n",
    "        self.use_resnet= use_resnet\n",
    "\n",
    "        if use_resnet:\n",
    "            self.block_res= BasicBlock(in_channels, in_channels, stride=1, downsample=None)\n",
    "        else:\n",
    "            self.first_layer= nn.Sequential(\n",
    "                nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(in_channels)\n",
    "            )\n",
    "            self.second_layer=nn.Sequential(\n",
    "                nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=3),\n",
    "                nn.BatchNorm2d(in_channels)\n",
    "            )\n",
    "            self.relu= nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        if self.use_resnet:\n",
    "            return self.block_res(x)\n",
    "        else:\n",
    "            identity= x\n",
    "            out= self.first_layer(x)\n",
    "            out= self.relu(x)\n",
    "            out= self.second_layer(x)\n",
    "            out= out  + identity\n",
    "            return self.relu(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54a9be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Block(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(CNN_Block, self).__init__()\n",
    "        self.block= nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.block(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "332f5550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Customize(nn.Module):\n",
    "    def __init__(self,depth, in_channels,out_channels, num_classes, use_skip, use_resnet ):\n",
    "        super(CNN_Customize, self).__init__()\n",
    "        \n",
    "        self.head= nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        blocks=[]\n",
    "\n",
    "        for i in range(depth):\n",
    "            if use_skip:\n",
    "                blocks.append(Residual_Block_CNN(out_channels, use_resnet))\n",
    "            else:\n",
    "                blocks.append(CNN_Block(out_channels))\n",
    "\n",
    "        self.blocks= nn.Sequential(*blocks)\n",
    "        self.pooling= nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fully_connected= nn.Linear(out_channels, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out= self.head(x)\n",
    "        out= self.blocks(out)\n",
    "        out= self.pooling(out)\n",
    "        out= torch.flatten(out, 1)\n",
    "        return self.fully_connected(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8baa0e-b17f-4a77-8a88-dadfdc6763ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "now= datetime.datetime.now()\n",
    "data_ora_formattata = now.strftime(\"%d_%m_%yT%H_%M\")\n",
    "name= f'run_{data_ora_formattata}'\n",
    "\n",
    "in_channels = 3\n",
    "out_channels= 64\n",
    "depths = [2 ,6 ,10]\n",
    "num_classes=10\n",
    "cifar_train, cifartest= Load_data_Cifar10()\n",
    "\n",
    "for depth in depths:\n",
    "    for use_skip in [True,False]:\n",
    "\n",
    "        if use_skip:\n",
    "            print(f'Run Training of Residual_CNN{depth} ')\n",
    "            logdir= f'tensorboard/CNN_Residual_vs_Base/{name}/Residual_depth{depth}'\n",
    "            path=f\"CNN_Residual_vs_Base/Residual_depth{depth}\"\n",
    "            \n",
    "            use_res=True\n",
    "        else:\n",
    "            print(f'Run Training of Simple_depth{depth} ')\n",
    "            logdir= f'tensorboard/CNN_Residual_vs_Base/{name}/Simple_depth{depth}'\n",
    "            path=f\"CNN_Residual_vs_Base/Simple_depth{depth}\"\n",
    "            \n",
    "            use_res=False\n",
    "        model= CNN_Customize(depth,in_channels,out_channels,num_classes,use_skip,use_res)\n",
    "        trainer= Trainer(model,logdir,data_ora_formattata,num_classes,0,depth,100,128,0.001,0.001,path)\n",
    "\n",
    "        trainer.Train(cifar_train)\n",
    "        trainer.Test(cifartest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4de2f2-abc5-4f98-9eaf-3497f734a022",
   "metadata": {},
   "source": [
    "-----\n",
    "## Exercise 2: Choose at Least One\n",
    "\n",
    "Below are **three** exercises that ask you to deepen your understanding of Deep Networks for visual recognition. You must choose **at least one** of the below for your final submission -- feel free to do **more**, but at least **ONE** you must submit. Each exercise is designed to require you to dig your hands **deep** into the guts of your models in order to do new and interesting things.\n",
    "\n",
    "**Note**: These exercises are designed to use your small, custom CNNs and small datasets. This is to keep training times reasonable. If you have a decent GPU, feel free to use pretrained ResNets and larger datasets (e.g. the [Imagenette](https://pytorch.org/vision/0.20/generated/torchvision.datasets.Imagenette.html#torchvision.datasets.Imagenette) dataset at 160px)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07978e8e-9f2e-4949-9699-495af6cb6349",
   "metadata": {},
   "source": [
    "### Exercise 2.1: *Fine-tune* a pre-trained model\n",
    "Train one of your residual CNN models from Exercise 1.3 on CIFAR-10. Then:\n",
    "1. Use the pre-trained model as a **feature extractor** (i.e. to extract the feature activations of the layer input into the classifier) on CIFAR-100. Use a **classical** approach (e.g. Linear SVM, K-Nearest Neighbor, or Bayesian Generative Classifier) from scikit-learn to establish a **stable baseline** performance on CIFAR-100 using the features extracted using your CNN.\n",
    "2. Fine-tune your CNN on the CIFAR-100 training set and compare with your stable baseline. Experiment with different strategies:\n",
    "    - Unfreeze some of the earlier layers for fine-tuning.\n",
    "    - Test different optimizers (Adam, SGD, etc.).\n",
    "\n",
    "Each of these steps will require you to modify your model definition in some way. For 1, you will need to return the activations of the last fully-connected layer (or the global average pooling layer). For 2, you will need to replace the original, 10-class classifier with a new, randomly-initialized 100-class classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa099a7",
   "metadata": {},
   "source": [
    "## Best model\n",
    "Residual10 is the best model finded in the previous train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "736fe51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_data_Cifar100():\n",
    "    transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    train_cifar100 = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "    test_cifar100 = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    return train_cifar100,test_cifar100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ed24691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load residual10 like path\n",
    "#Per caricare il modello e utilizzarlo come un extract features si puo prendere sicuramente fino all' ultimo layer e sicuramente devo portarlo a 100\n",
    "# essendo che le classi ora da predirre sono 100\n",
    "# dopo aver visto infatti la struttura sappiamo dunque di dover cambiare l'ultimo fc\n",
    "def Load_model(path,in_channels, out_channels, verbose=False):\n",
    "    hyperparam= pd.read_csv(path+\"/hyperparametres.csv\")\n",
    "    model = CNN_Customize(\n",
    "        hyperparam[\"Depth\"][0],\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        hyperparam[\"Num Classes\"][0],\n",
    "        True,\n",
    "        True\n",
    "    )\n",
    "    model.load_state_dict(torch.load(path+\"/best_model.pt\", map_location=\"cpu\"))\n",
    "    if verbose:\n",
    "        print(model)\n",
    "    return model, hyperparam\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9039eb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model ,h=Load_model(\"CNN_Residual_vs_Base/Residual_depth6/Run_10_09_25T09_41\",3, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7c39a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_Customize(\n",
       "  (head): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (blocks): Sequential(\n",
       "    (0): Residual_Block_CNN(\n",
       "      (block_res): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Residual_Block_CNN(\n",
       "      (block_res): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Residual_Block_CNN(\n",
       "      (block_res): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): Residual_Block_CNN(\n",
       "      (block_res): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): Residual_Block_CNN(\n",
       "      (block_res): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Residual_Block_CNN(\n",
       "      (block_res): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooling): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fully_connected): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "181c2d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_configuration():\n",
    "    return {\n",
    "    \"adam\": {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 5e-5,\n",
    "        \"momentum\": None  # non serve per AdamW\n",
    "    },\n",
    "    \"adamw\": {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 5e-5,\n",
    "        \"momentum\": None  # non serve per AdamW\n",
    "    },\n",
    "    \"sgd\": {\n",
    "        \"lr\": 1e-4,\n",
    "        \"weight_decay\": 5e-4,\n",
    "        \"momentum\": 0.9\n",
    "    },\n",
    "    \"rmsprop\": {\n",
    "        \"lr\": 1e-5,\n",
    "        \"weight_decay\": 5e-4,\n",
    "        \"momentum\": 0.9\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13960691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine_Tuning model with unfreeze layers with optimizer adamw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0: 100%|██████████| 157/157 [00:23<00:00,  6.57it/s]\n",
      "Model Training:   3%|▎         | 1/30 [00:26<12:58, 26.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.118976825361798 of Epoch 0\n",
      "Validation Loss: 3.7338462710380553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1: 100%|██████████| 157/157 [00:23<00:00,  6.55it/s]\n",
      "Model Training:   7%|▋         | 2/30 [00:53<12:32, 26.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.4288156533696847 of Epoch 1\n",
      "Validation Loss: 3.275784319639206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 2: 100%|██████████| 157/157 [00:24<00:00,  6.52it/s]\n",
      "Model Training:  10%|█         | 3/30 [01:20<12:07, 26.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.0963605832142433 of Epoch 2\n",
      "Validation Loss: 3.12365984916687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 3: 100%|██████████| 157/157 [00:24<00:00,  6.52it/s]\n",
      "Model Training:  13%|█▎        | 4/30 [01:47<11:41, 26.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.883369429096295 of Epoch 3\n",
      "Validation Loss: 2.9126761198043822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 4: 100%|██████████| 157/157 [00:24<00:00,  6.51it/s]\n",
      "Model Training:  17%|█▋        | 5/30 [02:14<11:15, 27.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.727861439346508 of Epoch 4\n",
      "Validation Loss: 2.852998453378677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 5: 100%|██████████| 157/157 [00:24<00:00,  6.50it/s]\n",
      "Model Training:  20%|██        | 6/30 [02:42<10:49, 27.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.5991322827187315 of Epoch 5\n",
      "Validation Loss: 2.7880787432193754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 6: 100%|██████████| 157/157 [00:24<00:00,  6.53it/s]\n",
      "Model Training:  23%|██▎       | 7/30 [03:08<10:21, 27.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.4978003623379266 of Epoch 6\n",
      "Validation Loss: 2.6989441245794294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 7: 100%|██████████| 157/157 [00:24<00:00,  6.47it/s]\n",
      "Model Training:  27%|██▋       | 8/30 [03:36<09:55, 27.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.404663290947106 of Epoch 7\n",
      "Validation Loss: 2.7283488154411315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 8: 100%|██████████| 157/157 [00:24<00:00,  6.49it/s]\n",
      "Model Training:  30%|███       | 9/30 [04:03<09:28, 27.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.3285213412752577 of Epoch 8\n",
      "Validation Loss: 2.640622079372406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 9: 100%|██████████| 157/157 [00:24<00:00,  6.49it/s]\n",
      "Model Training:  33%|███▎      | 10/30 [04:30<09:02, 27.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.258401787205107 of Epoch 9\n",
      "Validation Loss: 2.5739482820034025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 10: 100%|██████████| 157/157 [00:24<00:00,  6.48it/s]\n",
      "Model Training:  37%|███▋      | 11/30 [04:57<08:35, 27.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.185656584751834 of Epoch 10\n",
      "Validation Loss: 2.563878732919693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 11: 100%|██████████| 157/157 [00:24<00:00,  6.47it/s]\n",
      "Model Training:  40%|████      | 12/30 [05:24<08:08, 27.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.126405614956169 of Epoch 11\n",
      "Validation Loss: 2.5212108105421067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 12: 100%|██████████| 157/157 [00:24<00:00,  6.40it/s]\n",
      "Model Training:  43%|████▎     | 13/30 [05:52<07:43, 27.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.069884931205944 of Epoch 12\n",
      "Validation Loss: 2.5524406284093857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 13: 100%|██████████| 157/157 [00:24<00:00,  6.46it/s]\n",
      "Model Training:  47%|████▋     | 14/30 [06:19<07:15, 27.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.0137039582440805 of Epoch 13\n",
      "Validation Loss: 2.5063811212778093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 14: 100%|██████████| 157/157 [00:24<00:00,  6.47it/s]\n",
      "Model Training:  50%|█████     | 15/30 [06:46<06:48, 27.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.9646556202773076 of Epoch 14\n",
      "Validation Loss: 2.4480356931686402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 15: 100%|██████████| 157/157 [00:24<00:00,  6.49it/s]\n",
      "Model Training:  53%|█████▎    | 16/30 [07:13<06:21, 27.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.9128527504623316 of Epoch 15\n",
      "Validation Loss: 2.448612341284752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 16: 100%|██████████| 157/157 [00:24<00:00,  6.49it/s]\n",
      "Model Training:  57%|█████▋    | 17/30 [07:41<05:53, 27.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.8657661524547893 of Epoch 16\n",
      "Validation Loss: 2.412120279669762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 17: 100%|██████████| 157/157 [00:24<00:00,  6.49it/s]\n",
      "Model Training:  60%|██████    | 18/30 [08:08<05:25, 27.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.8222499174676883 of Epoch 17\n",
      "Validation Loss: 2.3689457356929777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 18: 100%|██████████| 157/157 [00:24<00:00,  6.46it/s]\n",
      "Model Training:  63%|██████▎   | 19/30 [08:35<04:59, 27.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.7757355399951813 of Epoch 18\n",
      "Validation Loss: 2.450173607468605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 19: 100%|██████████| 157/157 [00:24<00:00,  6.46it/s]\n",
      "Model Training:  67%|██████▋   | 20/30 [09:02<04:32, 27.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.7333954754908374 of Epoch 19\n",
      "Validation Loss: 2.4792622357606886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 20: 100%|██████████| 157/157 [00:24<00:00,  6.49it/s]\n",
      "Model Training:  70%|███████   | 21/30 [09:29<04:04, 27.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.6946298172519465 of Epoch 20\n",
      "Validation Loss: 2.4320870101451875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 21: 100%|██████████| 157/157 [00:24<00:00,  6.48it/s]\n",
      "Model Training:  73%|███████▎  | 22/30 [09:56<03:37, 27.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.6511513602202106 of Epoch 21\n",
      "Validation Loss: 2.4833213865756987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 22: 100%|██████████| 157/157 [00:24<00:00,  6.50it/s]\n",
      "Model Training:  77%|███████▋  | 23/30 [10:23<03:10, 27.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.6174033333541482 of Epoch 22\n",
      "Validation Loss: 2.4244469553232193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 23: 100%|██████████| 157/157 [00:24<00:00,  6.50it/s]\n",
      "Model Training:  80%|████████  | 24/30 [10:51<02:42, 27.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.572215260973402 of Epoch 23\n",
      "Validation Loss: 2.3907342731952665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 24: 100%|██████████| 157/157 [00:24<00:00,  6.47it/s]\n",
      "Model Training:  83%|████████▎ | 25/30 [11:18<02:15, 27.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.5457578138181358 of Epoch 24\n",
      "Validation Loss: 2.5468485176563265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 25: 100%|██████████| 157/157 [00:24<00:00,  6.49it/s]\n",
      "Model Training:  87%|████████▋ | 26/30 [11:45<01:48, 27.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.5076786856742421 of Epoch 25\n",
      "Validation Loss: 2.3648193001747133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 26: 100%|██████████| 157/157 [00:24<00:00,  6.49it/s]\n",
      "Model Training:  90%|█████████ | 27/30 [12:12<01:21, 27.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.4768184757536384 of Epoch 26\n",
      "Validation Loss: 2.5154344886541367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 27:   0%|          | 0/157 [00:00<?, ?it/s]\n",
      "Model Training:  90%|█████████ | 27/30 [12:12<01:21, 27.14s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 45\u001b[0m\n\u001b[1;32m     41\u001b[0m                 path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReusing_Model/Fine_Tuning_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/Optimizer_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     42\u001b[0m             trainer\u001b[38;5;241m=\u001b[39m Trainer(model,logdirs,data_ora_formattata,num_classes,depth,\u001b[38;5;241m30\u001b[39m,\u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m     43\u001b[0m                              config_optim[optim][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],config_optim[optim][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     44\u001b[0m                              path,\u001b[38;5;28;01mFalse\u001b[39;00m,freeze_layers,\u001b[38;5;28;01mNone\u001b[39;00m,optim,config_optim[optim][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmomentum\u001b[39m\u001b[38;5;124m\"\u001b[39m],block_unfreeze)\n\u001b[0;32m---> 45\u001b[0m             trainer\u001b[38;5;241m.\u001b[39mFine_Tuning(cifar_train,cifar_test)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinsh Fine Tuning\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 99\u001b[0m, in \u001b[0;36mTrainer.Fine_Tuning\u001b[0;34m(self, X_train, X_test)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfine_tuning\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m=\u001b[39m result\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTrain(X_train)\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTest(X_test)\n",
      "Cell \u001b[0;32mIn[14], line 79\u001b[0m, in \u001b[0;36mTrainer.Train\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_hyperparametres()\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_model\u001b[38;5;241m=\u001b[39m Training_Model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_writer,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_decay)\n\u001b[1;32m     80\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_model\u001b[38;5;241m.\u001b[39mstate_dict(), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_experiments,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.pt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[27], line 37\u001b[0m, in \u001b[0;36mTraining_Model\u001b[0;34m(model, X, file_writer, device, optimizer, epochs, batch_size, learning_rate, weight_decay, study_grad)\u001b[0m\n\u001b[1;32m     35\u001b[0m     count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     36\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 37\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     39\u001b[0m loss_average\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(losses)\n\u001b[1;32m     42\u001b[0m accurancy, report_dict, losses_val\u001b[38;5;241m=\u001b[39m Validation_Model(model, ds_val, device, batch_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "now= datetime.datetime.now()\n",
    "data_ora_formattata = now.strftime(\"%d_%m_%yT%H_%M\")\n",
    "name= f'run_{data_ora_formattata}'\n",
    "\n",
    "path_model_CNN= \"CNN_Residual_vs_Base/Residual_depth6/Run_10_09_25T09_41\"\n",
    "in_channels = 3\n",
    "out_channels= 64\n",
    "depth =6\n",
    "num_classes= 100\n",
    "\n",
    "model, hyperparametres= Load_model(path_model_CNN, in_channels, out_channels)\n",
    "block_unfreeze = [ \"head\",\"blocks.0\",\"blocks.1\", \"fully_connected\"]\n",
    "optimizer=[\"adamw\", \"sgd\", \"rmsprop\",\"adam\"]\n",
    "classificator=[\"svm\", \"knn\", \"gaussian\"]\n",
    "\n",
    "cifar_train ,cifar_test= Load_data_Cifar100()\n",
    "\n",
    "config_optim=Load_configuration()\n",
    "\n",
    "\n",
    "for freeze_layers in [True]:\n",
    "    if freeze_layers==False:\n",
    "        for cl in classificator:\n",
    "            clear_output(wait=True)\n",
    "            print(f'BaseLine with CNN Extract Feature with classificator: {cl}')\n",
    "            logdirs= f'tensorboard/Reusing_Model/Classification_{name}/Classificator_{cl}'\n",
    "            path= f'Reusing_Model/Classification_{name}/Classificator_{cl}'\n",
    "            trainer= Trainer(model,logdirs,data_ora_formattata,num_classes,depth,0,128,freeze_layers=freeze_layers,classificator=cl)\n",
    "            trainer.Fine_Tuning(cifar_train,cifar_test)\n",
    "    else:\n",
    "        for optim in optimizer:\n",
    "            clear_output(wait=True)\n",
    "            model, hyperparametres= Load_model(path_model_CNN, in_channels, out_channels)\n",
    "            if optim==\"adam\":\n",
    "                print(f'Fine tuning CNN model only with unfreeze last layers')\n",
    "                logdirs= f'tensorboard/Reusing_Model/Fine_Tuning_{name}/Unfreeze_last_layers'\n",
    "                path=f'Reusing_Model/Fine_Tuning_{name}/Unfreeze_last_layers'\n",
    "            else:\n",
    "                print(f'Fine_Tuning model with unfreeze layers with optimizer {optim}')\n",
    "                logdirs= f'tensorboard/Reusing_Model/Fine_Tuning_{name}/Optimizer_{optim}'\n",
    "                path=f'Reusing_Model/Fine_Tuning_{name}/Optimizer_{optim}'\n",
    "            trainer= Trainer(model,logdirs,data_ora_formattata,num_classes,depth,30,256,\n",
    "                             config_optim[optim][\"lr\"],config_optim[optim][\"weight_decay\"],\n",
    "                             path,False,freeze_layers,None,optim,config_optim[optim][\"momentum\"],block_unfreeze)\n",
    "            trainer.Fine_Tuning(cifar_train,cifar_test)\n",
    "\n",
    "print(\"finsh Fine Tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83542a31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

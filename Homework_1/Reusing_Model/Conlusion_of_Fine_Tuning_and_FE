Fine-tuning the pre-trained residual CNN on CIFAR-10 for CIFAR-100 generally does not exceed 30–40% accuracy.
This is mainly due to two reasons. 
First, the network has all blocks with only 64 channels, so its representational capacity is limited and cannot effectively discriminate among 100 classes, many of which are similar. Second, the features learned on CIFAR-10 were optimized to distinguish 10 very different classes and are not sufficiently general for the new dataset. During fine-tuning, gradients in the early layers are very small, so the base features change little, limiting the network’s adaptation. As a result, even using the extracted features for classical classifiers, performance remains low. In summary, both the limited network capacity and the poorly adaptable features explain why fine-tuning on CIFAR-100 with this model does not work well.
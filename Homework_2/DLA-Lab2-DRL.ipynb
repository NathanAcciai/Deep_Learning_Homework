{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6ae2782-57ea-48f0-b948-b412d0076ffc",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning Laboratory\n",
    "\n",
    "In this laboratory session we will work on getting more advanced versions of Deep Reinforcement Learning algorithms up and running. Deep Reinforcement Learning is **hard**, and getting agents to stably train can be frustrating and requires quite a bit of subtlety in analysis of intermediate results. We will start by refactoring (a bit) my implementation of `REINFORCE` on the [Cartpole environment](https://gymnasium.farama.org/environments/classic_control/cart_pole/). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fadaf0c-f9b2-4680-8456-0eadb5eb8c2f",
   "metadata": {},
   "source": [
    "## Exercise 1: Improving my `REINFORCE` Implementation (warm up)\n",
    "\n",
    "In this exercise we will refactor a bit and improve some aspects of my `REINFORCE` implementation. \n",
    "\n",
    "**First Things First**: Spend some time playing with the environment to make sure you understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "241e9bdc-7aa9-4a12-a57f-088fdc87eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Instantiate a rendering and a non-rendering environment.\n",
    "env_render = gym.make('CartPole-v1', render_mode='human')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168989e9",
   "metadata": {},
   "source": [
    "CartPole è un carrello che può muoversi a sinistra o a destra su un binario, sopra il carello è montato un pali collegato con una cerniera alla base, l'agente deve bilanciare il palo muovendo il carello \n",
    "Stati (osservazioni):\n",
    "1) posizione del carello\n",
    "2) velocità del carello \n",
    "3) angolo del palo\n",
    "4) velocità angolare del palo \n",
    "\n",
    "Azioni: \n",
    "0) spinge il carello a sinistra\n",
    "1) spinge il carello a destra\n",
    "\n",
    "Ricompensa:\n",
    "Se il palo rimane in piedi allora avro una reward +1\n",
    "\n",
    "Condizioni di fine episodio\n",
    "1) angolo troppo grande: Il palo cade\n",
    "2) carello esce dai limiti di pista\n",
    "3) si supera 500 passi\n",
    "\n",
    "Obbiettivo: imaprare una politica che muova il carello in modo da non far cadere il palo il più a lungo possibile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbd2dd9d-a60b-4892-878f-d83f64ac5f63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation after the reset: [ 0.0149404  -0.0409965  -0.01206485 -0.03211079]\n",
      "information of action {}\n",
      "Observation space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "Action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "obs, info= env.reset()\n",
    "\n",
    "print(f'Observation after the reset: {obs}')\n",
    "print(f'information of action {info}')\n",
    "\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7817a2f8",
   "metadata": {},
   "source": [
    "observation space: [posizione carello, velocità carello, angolo del palo, velocità angolare del palo]\n",
    "action space: (discreto) [0 sposta a sinistra, 1 sposta a destra]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae31dce-be71-4a86-95fd-1af902f026b8",
   "metadata": {},
   "source": [
    "**Next Things Next**: Now get your `REINFORCE` implementation working on the environment. You can import my (probably buggy and definitely inefficient) implementation here. Or even better, refactor an implementation into a separate package from which you can `import` the stuff you need here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b8a90f-78ed-4f43-9d66-aeba7c74fe2c",
   "metadata": {},
   "source": [
    "**Last Things Last**: My implementation does a **super crappy** job of evaluating the agent performance during training. The running average is not a very good metric. Modify my implementation so that every $N$ iterations (make $N$ an argument to the training function) the agent is run for $M$ episodes in the environment. Collect and return: (1) The average **total** reward received over the $M$ iterations; and (2) the average episode length. Analyze the performance of your agents with these new metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c722e50-f882-430f-903e-8a97d38bd0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import gymnasium as gym\n",
    "#import torch \n",
    "#import torch.nn as nn\n",
    "#import torch.optim as optim\n",
    "#from torch.distributions import Categorical\n",
    "#import torch.nn.functional as F\n",
    "#import matplotlib.pyplot as plt\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "#import os\n",
    "#import datetime\n",
    "#from reinforce_cartpole import PolicyNetwork, ReinforceAgent, TrainAgentRenforce\n",
    "##import pygame\n",
    "##_ = pygame.init()\n",
    "#\n",
    "#now= datetime.datetime.now()\n",
    "#data_ora_formattata = now.strftime(\"%d_%m_%yT%H_%M\")\n",
    "#name= f'run_{data_ora_formattata}'\n",
    "#\n",
    "##env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "#env = gym.make(\"CartPole-v1\")\n",
    "##pygame.display.init() \n",
    "#name_agent=\"CartPole_REINFORCE\"\n",
    "#temperature_train=0.7\n",
    "#general_path= f'Reinforcment_Learning/{name_agent}_{data_ora_formattata}_temp_{temperature_train}'\n",
    "#\n",
    "#checkpoint_path=general_path+\"/checkpoint\"\n",
    "#bestmodel_path= general_path+\"/best_model\"\n",
    "#hyperparamtres_path= general_path+\"/hyperparametres\"\n",
    "#\n",
    "#obs_dim = env.observation_space.shape[0]\n",
    "#action_dim = env.action_space.n\n",
    "#\n",
    "#policy = PolicyNetwork(obs_dim=obs_dim, action_dim=action_dim)\n",
    "#\n",
    "#logdir= f'tensorboard/Reinforcment_Learning/{name_agent}/{name}_temp_{temperature_train}'\n",
    "#\n",
    "#agent = ReinforceAgent(\n",
    "#    enviroment=env,\n",
    "#    logdir=logdir, #da modificare\n",
    "#    policy=policy,\n",
    "#    name_agent=name_agent,\n",
    "#    gamma=0.99,\n",
    "#    max_lenght=500\n",
    "#)\n",
    "#\n",
    "#trainer = TrainAgentRenforce(\n",
    "#    reinforcagent=agent,\n",
    "#    lr=1e-2,\n",
    "#    num_episode=500,\n",
    "#    num_episode_validation=10,\n",
    "#    check_val=10,\n",
    "#    checkpoint_path=checkpoint_path,\n",
    "#    best_model_path=bestmodel_path,\n",
    "#    hyperparams_path=hyperparamtres_path,\n",
    "#    temperature_train=temperature_train\n",
    "#)\n",
    "#\n",
    "##try:\n",
    "#running_rewards = trainer.train_agent()\n",
    "##finally:\n",
    "##    env.close()\n",
    "##    pygame.display.quit()\n",
    "##    pygame.quit()\n",
    "#\n",
    "#\n",
    "##pygame.display.quit()\n",
    "#\n",
    "##da fare quella ccosa della finestra pygame e modificarla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d70187",
   "metadata": {},
   "source": [
    "## Risultati\n",
    "Dal codice di partenza fornito è stato pulito e diviso in varie componenti in modo tale da avere una maggiore generalizzazione delle classi.\n",
    "Successivamente sono state introdotte varie componenti aggiuntive:\n",
    "1) Un parametro di temperatura che sostanzialmente serve per il campionamento dalla distribuzione di probabilità nella scelta delle azioni.\n",
    "Più la temperatura è grande e più la distribuzione diventa uniforme e quindi si ha più esplorazione da parte del modello, al contrario più la temperatura è piccola più il modello sceglie azioni deterministiche selezionando sempre quella migliore\n",
    "2) introduzione di varie metriche: \n",
    "    1) Lunghezza dell' episodio: Serve per misurare le performance e la stabilità del modello, infatti è gradevole avere un modello che dopo t iterazioni ha episodi più lunghi fino ad completarli.\n",
    "    2) Deviazione standard: Valuta la stabilità del modello, se risulta alta oppure altalenante vuol dire che il modello non è del tutto stabile, mentre quando si ha una deviazione standard che tende a diminuire significa che il modello tende a prendere le stesse decisioni.\n",
    "    3) Min reward: misura la ricompensa minima che si ha nell' episodio, è un ottimo metodo per capire se il modello risulta essere stabile e efficiente all' interno dell' ambiente infatti una volta che il modello impara ci si aspetta una min reward sempre più alta.\n",
    "    4) max reward: Misura l'efficienza del modello, infatti serve a dare una stima della ricompensa massima ottenuta dal modello \n",
    "    5) mean reward: Misura la ricompensa media anche questo da sintomi di stabilità o instabilità infatti, la cosa che ci aspettiamo è che essa salga poichè significherebbe che il modello mediamente compiè azioni che hanno un alta ricompensa.\n",
    "    6) termination failure/success: Rappresentano il numero di successi/fallimenti all' interno dell' addestramento, questo ci da una misura di quanto il modello poi riesca o meno a portare a termine l'obbiettivo\n",
    "    7) percentile 10%: serve per valutare gli episodi peggiori che stanno sotto al 10%. ci da una misura di quanto il modello durante l'addestramento riesca a migliorarsi quindi a portare il 10% degli episodi peggiori comunque a delle ottime metriche.\n",
    "\n",
    "Sono stati fatti vari addestramenti cercando di andare a modificare la temperatura e non introducendo alcuna baseline (essendo esercizio successivo).\n",
    "Il range di temperatura preso è stato:\n",
    "1: distribuzione morbida equilibrata tra determinismo e stocasticità \n",
    "2: distribuzione quasi uniforme molto piatta quindi qausi solamente esplorazione\n",
    "0.3-0.5-0.7: distribuzione più deterministica il modello si concentra sul' prendere azioni migliori, poca esplorazione\n",
    "\n",
    "Il risultato ottenuto è che la temperatura a 0.5 risulti essere effettivamente la migliore anche se porta con se una certa instabilità che si riscontra sia nella reward sia nella deviazione standard. ma comunque è l'unica che riesce a ottenere i risultati migliori"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5adad7-759b-4000-925b-701f41fe6e97",
   "metadata": {},
   "source": [
    "-----\n",
    "## Exercise 2: `REINFORCE` with a Value Baseline (warm up)\n",
    "\n",
    "In this exercise we will augment my implementation (or your own) of `REINFORCE` to subtract a baseline from the target in the update equation in order to stabilize (and hopefully speed-up) convergence. For now we will stick to the Cartpole environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdd5bc3-a439-4d84-891b-f4a840331e07",
   "metadata": {},
   "source": [
    "**First Things First**: Recall from the slides on Deep Reinforcement Learning that we can **subtract** any function that doesn't depend on the current action from the q-value without changing the (maximum of our) objecttive function $J$:  \n",
    "\n",
    "$$ \\nabla J(\\boldsymbol{\\theta}) \\propto \\sum_{s} \\mu(s) \\sum_a \\left( q_{\\pi}(s, a) - b(s) \\right) \\nabla \\pi(a \\mid s, \\boldsymbol{\\theta}) $$\n",
    "\n",
    "In `REINFORCE` this means we can subtract from our target $G_t$:\n",
    "\n",
    "$$ \\boldsymbol{\\theta}_{t+1} \\triangleq \\boldsymbol{\\theta}_t + \\alpha (G_t - b(S_t)) \\frac{\\nabla \\pi(A_t \\mid s, \\boldsymbol{\\theta})}{\\pi(A_t \\mid s, \\boldsymbol{\\theta})} $$\n",
    "\n",
    "Since we are only interested in the **maximum** of our objective, we can also **rescale** our target by any function that also doesn't depend on the action. A **simple baseline** which is even independent of the state -- that is, it is **constant** for each episode -- is to just **standardize rewards within the episode**. So, we **subtract** the average return and **divide** by the variance of returns:\n",
    "\n",
    "$$ \\boldsymbol{\\theta}_{t+1} \\triangleq \\boldsymbol{\\theta}_t + \\alpha \\left(\\frac{G_t - \\bar{G}}{\\sigma_G}\\right) \\nabla  \\pi(A_t \\mid s, \\boldsymbol{\\theta}) $$\n",
    "\n",
    "This baseline is **already** implemented in my implementation of `REINFORCE`. Experiment with and without this standardization baseline and compare the performance. We are going to do something more interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf5b245-9369-4ed9-b5f4-3a52bb975d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import gymnasium as gym\n",
    "#import torch \n",
    "#import torch.nn as nn\n",
    "#import torch.optim as optim\n",
    "#from torch.distributions import Categorical\n",
    "#import torch.nn.functional as F\n",
    "#import matplotlib.pyplot as plt\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "#import os\n",
    "#import datetime\n",
    "#from reinforce_cartpole import PolicyNetwork, ReinforceAgent, TrainAgentRenforce\n",
    "##import pygame\n",
    "##_ = pygame.init()\n",
    "#\n",
    "#now= datetime.datetime.now()\n",
    "#data_ora_formattata = now.strftime(\"%d_%m_%yT%H_%M\")\n",
    "#name= f'run_{data_ora_formattata}'\n",
    "#\n",
    "##env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "#env = gym.make(\"CartPole-v1\")\n",
    "##pygame.display.init() \n",
    "#name_agent=\"CartPole_REINFORCE\"\n",
    "#normalizzation_discount=False\n",
    "#baseline_discount=False\n",
    "#temperature_train=[0.3, 0.5, 0.7, 1 , 1.5, 2]\n",
    "#for temp in temperature_train:\n",
    "#    for subtract in [True,False]:\n",
    "#        baseline= \"simple_subtract\" if subtract else \"normalization\"\n",
    "#\n",
    "#\n",
    "#        general_path= f'Reinforcment_Learning_Classic_Baseline_sub_std/{name_agent}_{data_ora_formattata}_temp_{temp}_Baseline_{baseline}'\n",
    "#\n",
    "#        checkpoint_path=general_path+\"/checkpoint\"\n",
    "#        bestmodel_path= general_path+\"/best_model\"\n",
    "#        hyperparamtres_path= general_path+\"/hyperparametres\"\n",
    "#\n",
    "#        obs_dim = env.observation_space.shape[0]\n",
    "#        action_dim = env.action_space.n\n",
    "#\n",
    "#        policy = PolicyNetwork(obs_dim=obs_dim, action_dim=action_dim)\n",
    "#\n",
    "#        logdir= f'tensorboard/Reinforcment_Learning_Baseline_sub_std/{name_agent}/{name}_temp_{temp}_BaseLine_{baseline}'\n",
    "#\n",
    "#        agent = ReinforceAgent(\n",
    "#            enviroment=env,\n",
    "#            logdir=logdir, #da modificare\n",
    "#            policy=policy,\n",
    "#            gamma=0.99,\n",
    "#            max_lenght=500\n",
    "#        )\n",
    "#\n",
    "#        trainer = TrainAgentRenforce(\n",
    "#            reinforcagent=agent,\n",
    "#            lr=1e-2,\n",
    "#            num_episode=500,\n",
    "#            num_episode_validation=10,\n",
    "#            check_val=10,\n",
    "#            checkpoint_path=checkpoint_path,\n",
    "#            best_model_path=bestmodel_path,\n",
    "#            hyperparams_path=hyperparamtres_path,\n",
    "#            temperature_train=temp\n",
    "#        )\n",
    "#        if baseline:\n",
    "#            running_rewards = trainer.train_agent(normalizzation_discount=False,baseline_discount_sub=True)\n",
    "#        else:\n",
    "#            running_rewards=trainer.train_agent(normalizzation_discount=True,baseline_discount_sub=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd4d345",
   "metadata": {},
   "source": [
    "## Risultati\n",
    "In questa parte di codice viene implementata la differenza tra due tipi di baseline:\n",
    "1) Baseline base dove si sottrae ai ritorni la media di essi.\n",
    "2) Baseline con standardizzazione dove si applica media e varianza dei ritorni.\n",
    "Questa tecniche servono per la stabilizzazione dei gradietni e dunque ad avere un allenamento più fluido e un modello più stabile.\n",
    "Queste due tecniche sonoi state trainate con lo stesso range di temperature viste sopra e il risultato ci mostra che non vi è un solo modello che mostra valore migliore.\n",
    "Si parte con la premessa che dagli esperimenti fatti si nota che la baseline che adotta la normalizzazione riesce ad avere dei gradienti più stabili e diciamo migliori performance in generale.\n",
    "Ci focalizziamo sopratutto su 4 modelli:\n",
    "1) temp_0.3_BaseLine_normalization\n",
    "2) temp_0.5_BaseLine_simple_subtract\n",
    "3) temp_0.7_BaseLine_normalization\n",
    "4) temp_0.7_BaseLine_simple_subtract\n",
    "Possiamo vedere questi risultati nel loro complesso dove la situazione risulta anche diciamo essere un po strana.\n",
    "Infatti si può vedere come i primi due modelli in Training riportino delle metriche migliori rispetto a gli ultimi due, anche se la situazione non è analoga per la validazione dove si ha quasi la situazione opposta.\n",
    "Quello che si può tirar fuori dalle metriche che si vedono e l'estrazione di due modelli migliori.\n",
    "Infatti si può vedere come due modelli spiccano tra tutti il resto che sono:\n",
    "1) temp_0.3_BaseLine_normalization\n",
    "2) temp_0.5_BaseLine_simple_subtract\n",
    "Infatti si può vedere come il loro comportamento risulti abbastanza analogo nei plot delle varie metriche, anche se in termini di stabilità sicuramente risulta esserlo maggiormente il 1 modello essendo che comunica attraverso i grafici una stabilità maggiore sia perchè dopo l'epoca 230 la deviazione standard risulta essere verametnte bassa, e le metriche in generale tendono a esseere molto stabili.\n",
    "Un' altra piccola differenza di stabilità si vede anche nei grafici di train sia per quanto riguarda la loss sia per quanto riguarda la reward media e ovviamenete dato anche dal maggior numero di successi.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cfde8e-f1c7-4d96-85e0-268f687e09df",
   "metadata": {},
   "source": [
    "**The Real Exercise**: Standard practice is to use the state-value function $v(s)$ as a baseline. This is intuitively appealing -- we are more interested in updating out policy for returns that estimate the current **value** worse. Our new update becomes:\n",
    "\n",
    "$$ \\boldsymbol{\\theta}_{t+1} \\triangleq \\boldsymbol{\\theta}_t + \\alpha (G_t - \\tilde{v}(S_t \\mid \\mathbf{w})) \\frac{\\nabla \\pi(A_t \\mid s, \\boldsymbol{\\theta})}{\\pi(A_t \\mid s, \\boldsymbol{\\theta})} $$\n",
    "\n",
    "where $\\tilde{v}(s \\mid \\mathbf{w})$ is a **deep neural network** with parameters $w$ that estimates $v_\\pi(s)$. What neural network? Typically, we use the **same** network architecture as that of the Policy.\n",
    "\n",
    "**Your Task**: Modify your implementation to fit a second, baseline network to estimate the value function and use it as **baseline**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de3a2a8-05c3-44e2-9858-8cc8f431325f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint not founded, start a new Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data01/dl24natacc/Deep_Learning_Application/Homework_2/reinforce_cartpole.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  obs= torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
      "/data01/dl24natacc/Deep_Learning_Application/Homework_2/reinforce_cartpole.py:326: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  returns= torch.tensor(self.reinforceagent.compute_discount_returns(rewards,normalizzation_discount,baseline_discount_sub), device= self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward train from episode 0: 18.0\n",
      "Average reward test from episode 0: 9.3\n",
      "\n",
      "Running reward train from episode 10: 9.0\n",
      "Average reward test from episode 10: 9.3\n",
      "\n",
      "Running reward train from episode 20: 11.0\n",
      "Average reward test from episode 20: 9.4\n",
      "\n",
      "Running reward train from episode 30: 9.0\n",
      "Average reward test from episode 30: 9.1\n",
      "\n",
      "Running reward train from episode 40: 10.0\n",
      "Average reward test from episode 40: 9.7\n",
      "\n",
      "Running reward train from episode 50: 10.0\n",
      "Average reward test from episode 50: 9.2\n",
      "\n",
      "Running reward train from episode 60: 10.0\n",
      "Average reward test from episode 60: 9.2\n",
      "\n",
      "Running reward train from episode 70: 10.0\n",
      "Average reward test from episode 70: 9.3\n",
      "\n",
      "Running reward train from episode 80: 11.0\n",
      "Average reward test from episode 80: 8.8\n",
      "\n",
      "Running reward train from episode 90: 10.0\n",
      "Average reward test from episode 90: 9.5\n",
      "\n",
      "Running reward train from episode 100: 11.0\n",
      "Average reward test from episode 100: 8.9\n",
      "\n",
      "Running reward train from episode 110: 10.0\n",
      "Average reward test from episode 110: 9.5\n",
      "\n",
      "Running reward train from episode 120: 8.0\n",
      "Average reward test from episode 120: 9.3\n",
      "\n",
      "Running reward train from episode 130: 11.0\n",
      "Average reward test from episode 130: 9.1\n",
      "\n",
      "Running reward train from episode 140: 10.0\n",
      "Average reward test from episode 140: 9.2\n",
      "\n",
      "Running reward train from episode 150: 12.0\n",
      "Average reward test from episode 150: 9.3\n",
      "\n",
      "Running reward train from episode 160: 10.0\n",
      "Average reward test from episode 160: 9.3\n",
      "\n",
      "Running reward train from episode 170: 9.0\n",
      "Average reward test from episode 170: 9.1\n",
      "\n",
      "Running reward train from episode 180: 9.0\n",
      "Average reward test from episode 180: 9.2\n",
      "\n",
      "Running reward train from episode 190: 11.0\n",
      "Average reward test from episode 190: 9.3\n",
      "\n",
      "Running reward train from episode 200: 10.0\n",
      "Average reward test from episode 200: 9.3\n",
      "\n",
      "Running reward train from episode 210: 10.0\n",
      "Average reward test from episode 210: 9.4\n",
      "\n",
      "Running reward train from episode 220: 14.0\n",
      "Average reward test from episode 220: 9.7\n",
      "\n",
      "Running reward train from episode 230: 10.0\n",
      "Average reward test from episode 230: 9.5\n",
      "\n",
      "Running reward train from episode 240: 14.0\n",
      "Average reward test from episode 240: 9.3\n",
      "\n",
      "Running reward train from episode 250: 11.0\n",
      "Average reward test from episode 250: 9.3\n",
      "\n",
      "Running reward train from episode 260: 12.0\n",
      "Average reward test from episode 260: 9.4\n",
      "\n",
      "Running reward train from episode 270: 11.0\n",
      "Average reward test from episode 270: 9.6\n",
      "\n",
      "Running reward train from episode 280: 11.0\n",
      "Average reward test from episode 280: 9.0\n",
      "\n",
      "Running reward train from episode 290: 10.0\n",
      "Average reward test from episode 290: 9.0\n",
      "\n",
      "Running reward train from episode 300: 9.0\n",
      "Average reward test from episode 300: 9.4\n",
      "\n",
      "Running reward train from episode 310: 9.0\n",
      "Average reward test from episode 310: 9.4\n",
      "\n",
      "Running reward train from episode 320: 10.0\n",
      "Average reward test from episode 320: 9.2\n",
      "\n",
      "Running reward train from episode 330: 10.0\n",
      "Average reward test from episode 330: 9.3\n",
      "\n",
      "Running reward train from episode 340: 9.0\n",
      "Average reward test from episode 340: 9.4\n",
      "\n",
      "Running reward train from episode 350: 9.0\n",
      "Average reward test from episode 350: 9.2\n",
      "\n",
      "Running reward train from episode 360: 21.0\n",
      "Average reward test from episode 360: 9.5\n",
      "\n",
      "Running reward train from episode 370: 11.0\n",
      "Average reward test from episode 370: 9.4\n",
      "\n",
      "Running reward train from episode 380: 24.0\n",
      "Average reward test from episode 380: 25.1\n",
      "\n",
      "Running reward train from episode 390: 37.0\n",
      "Average reward test from episode 390: 66.3\n",
      "\n",
      "Running reward train from episode 400: 33.0\n",
      "Average reward test from episode 400: 45.8\n",
      "\n",
      "Running reward train from episode 410: 46.0\n",
      "Average reward test from episode 410: 56.8\n",
      "\n",
      "Running reward train from episode 420: 149.0\n",
      "Average reward test from episode 420: 254.1\n",
      "\n",
      "Running reward train from episode 430: 500.0\n",
      "Average reward test from episode 430: 500.0\n",
      "\n",
      "Running reward train from episode 440: 284.0\n",
      "Average reward test from episode 440: 472.0\n",
      "\n",
      "Running reward train from episode 450: 446.0\n",
      "Average reward test from episode 450: 500.0\n",
      "\n",
      "Running reward train from episode 460: 316.0\n",
      "Average reward test from episode 460: 500.0\n",
      "\n",
      "Running reward train from episode 470: 240.0\n",
      "Average reward test from episode 470: 500.0\n",
      "\n",
      "Running reward train from episode 480: 248.0\n",
      "Average reward test from episode 480: 500.0\n",
      "\n",
      "Running reward train from episode 490: 500.0\n",
      "Average reward test from episode 490: 500.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import datetime\n",
    "from reinforce_cartpole import PolicyNetwork, ReinforceAgent, TrainAgentRenforce\n",
    "#import pygame\n",
    "#_ = pygame.init()\n",
    "\n",
    "now= datetime.datetime.now()\n",
    "data_ora_formattata = now.strftime(\"%d_%m_%yT%H_%M\")\n",
    "name= f'run_{data_ora_formattata}'\n",
    "\n",
    "#env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "#pygame.display.init() \n",
    "name_agent=\"CartPole_REINFORCE\"\n",
    "normalizzation_discount=False\n",
    "baseline_discount=False\n",
    "temperature_train=[0.3, 0.5, 0.7, 1 , 1.5, 2]\n",
    "for temp in [0.5]:\n",
    "\n",
    "    general_path= f'Reinforcment_Learning_Value_Net/{name_agent}_{data_ora_formattata}_temp_{temp}'\n",
    "\n",
    "    checkpoint_path=general_path+\"/checkpoint\"\n",
    "    bestmodel_path= general_path+\"/best_model\"\n",
    "    hyperparamtres_path= general_path+\"/hyperparametres\"\n",
    "\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    policy = PolicyNetwork(obs_dim=obs_dim, action_dim=action_dim)\n",
    "\n",
    "    logdir= f'tensorboard/Reinforcment_Learning_Value_Net/{name_agent}/{name}_temp_{temp}'\n",
    "\n",
    "    agent = ReinforceAgent(\n",
    "        enviroment=env,\n",
    "        logdir=logdir, \n",
    "        policy=policy,\n",
    "        gamma=0.99,\n",
    "        max_lenght=500,\n",
    "        value_net=True\n",
    "    )\n",
    "\n",
    "    trainer = TrainAgentRenforce(\n",
    "        reinforcagent=agent,\n",
    "        lr=1e-2,\n",
    "        num_episode=500,\n",
    "        num_episode_validation=10,\n",
    "        check_val=10,\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        best_model_path=bestmodel_path,\n",
    "        hyperparams_path=hyperparamtres_path,\n",
    "        temperature_train=temp\n",
    "    )\n",
    "    running_rewards = trainer.train_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bf1447-d222-4b24-a357-5b7f9824390c",
   "metadata": {},
   "source": [
    "-----\n",
    "## Exercise 3: Going Deeper\n",
    "\n",
    "As usual, pick **AT LEAST ONE** of the following exercises to complete.\n",
    "\n",
    "### Exercise 3.1: Solving Lunar Lander with `REINFORCE` (easy)\n",
    "\n",
    "Use my (or even better, improve on my) implementation of `REINFORCE` to solve the [Lunar Lander Environment](https://gymnasium.farama.org/environments/box2d/lunar_lander/). This environment is a little bit harder than Cartpole, but not much. Make sure you perform the same types of analyses we did during the lab session to quantify and qualify the performance of your agents.\n",
    "\n",
    "### Exercise 3.2: Solving Cartpole and Lunar Lander with `Deep Q-Learning` (harder)\n",
    "\n",
    "On policy Deep Reinforcement Learning tends to be **very unstable**. Write an implementation (or adapt an existing one) of `Deep Q-Learning` to solve our two environments (Cartpole and Lunar Lander). To do this you will need to implement a **Replay Buffer** and use a second, slow-moving **target Q-Network** to stabilize learning.\n",
    "\n",
    "### Exercise 3.3: Solving the OpenAI CarRacing environment (hardest) \n",
    "\n",
    "Use `Deep Q-Learning` -- or even better, an off-the-shelf implementation of **Proximal Policy Optimization (PPO)** -- to train an agent to solve the [OpenAI CarRacing](https://github.com/andywu0913/OpenAI-GYM-CarRacing-DQN) environment. This will be the most *fun*, but also the most *difficult*. Some tips:\n",
    "\n",
    "1. Make sure you use the `continuous=False` argument to the environment constructor. This ensures that the action space is **discrete** (we haven't seen how to work with continuous action spaces).\n",
    "2. Your Q-Network will need to be a CNN. A simple one should do, with two convolutional + maxpool layers, folowed by a two dense layers. You will **definitely** want to use a GPU to train your agents.\n",
    "3. The observation space of the environment is a single **color image** (a single frame of the game). Most implementations stack multiple frames (e.g. 3) after converting them to grayscale images as an observation.\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

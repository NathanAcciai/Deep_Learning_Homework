{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23bacf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, get_dataset_split_names\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "import matplotlib as plt\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import accelerate\n",
    "import sentencepiece\n",
    "import ipywidgets\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix,precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import json \n",
    "from datetime import datetime\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e91cdb-4693-4632-b7aa-f37eec027131",
   "metadata": {},
   "source": [
    "## Working with Transformers in the HuggingFace Ecosystem\n",
    "\n",
    "In this laboratory exercise we will learn how to work with the HuggingFace ecosystem to adapt models to new tasks. As you will see, much of what is required is *investigation* into the inner-workings of the HuggingFace abstractions. With a little work, a little trial-and-error, it is fairly easy to get a working adaptation pipeline up and running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e556105-269f-43e3-8933-227269afb9ea",
   "metadata": {},
   "source": [
    "### Exercise 1: Sentiment Analysis (warm up)\n",
    "\n",
    "In this first exercise we will start from a pre-trained BERT transformer and build up a model able to perform text sentiment analysis. Transformers are complex beasts, so we will build up our pipeline in several explorative and incremental steps.\n",
    "\n",
    "#### Exercise 1.1: Dataset Splits and Pre-trained model\n",
    "There are a many sentiment analysis datasets, but we will use one of the smallest ones available: the [Cornell Rotten Tomatoes movie review dataset](cornell-movie-review-data/rotten_tomatoes), which consists of 5,331 positive and 5,331 negative processed sentences from the Rotten Tomatoes movie reviews.\n",
    "\n",
    "**Your first task**: Load the dataset and figure out what splits are available and how to get them. Spend some time exploring the dataset to see how it is organized. Note that we will be using the [HuggingFace Datasets](https://huggingface.co/docs/datasets/en/index) library for downloading, accessing, splitting, and batching data for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96e48ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")\n",
    "train_dataset = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\", split=\"train\")\n",
    "valid_dataset = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\", split=\"validation\")\n",
    "test_dataset  = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\", split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27f50a0-0af4-4bdd-9a60-bab26f126c14",
   "metadata": {},
   "source": [
    "#### Exercise 1.2: A Pre-trained BERT and Tokenizer\n",
    "\n",
    "The model we will use is a *very* small BERT transformer called [Distilbert](https://huggingface.co/distilbert/distilbert-base-uncased) this model was trained (using self-supervised learning) on the same corpus as BERT but using the full BERT base model as a *teacher*.\n",
    "\n",
    "**Your next task**: Load the Distilbert model and corresponding tokenizer. Use the tokenizer on a few samples from the dataset and pass the tokens through the model to see what outputs are provided. I suggest you use the [`AutoModel`](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html) class (and the `from_pretrained()` method) to load the model and `AutoTokenizer` to load the tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eefc651-8551-44c2-8340-37d64660fc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cad8a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 52, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#estraggo del testo che mi fornisce il dataset di train \n",
    "texts= train_dataset[\"text\"][:2]\n",
    "#passo il testo delle due frasi selezionate al tokenizzatore di testo che restituisce inputs_ids e attention mask\n",
    "encoding= tokenizer(\n",
    "    texts,\n",
    "    padding= True,\n",
    "    truncation= True,\n",
    "    return_tensors=\"pt\" \n",
    ")\n",
    "#print(encoding)\n",
    "'''\n",
    "    inputs_ids rappresenta l'assegnazione numerica data dopo la tokenizzazione, infatti spezza la frase in token e restituisce \n",
    "    per ogni token gli id interi ripresi dal vocabolario del modello\n",
    "    mentre attention_mask indica quali token il modello deve cosniderare, così da ignorare il padding e evitare che il modello \n",
    "    presti attenzione a token finiti. in questo modo si ha più pulizia e minor rumore quando si calcola l 'attenzione all' interno del \n",
    "    mdoello.\n",
    "    '''\n",
    "    #In. questo caso si va a passare sia gli id che la maschera di attenzione al modello in modo elegante che estrae le feature\n",
    "with torch.no_grad():\n",
    "    outputs= model(**encoding)\n",
    "#print(outputs)\n",
    "    #l'ultimo layer mi restituisce la rappresentazione contestualizzata della frase ed è prioprio questo che viene usato come rappresentazione\n",
    "    #per classificare i serntimenti.\n",
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c776710c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"text\"][:2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71b41b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'the', 'gorgeous', '##ly', 'elaborate', 'continuation', 'of', '\"', 'the', 'lord', 'of', 'the', 'rings', '\"', 'trilogy', 'is', 'so', 'huge', 'that', 'a', 'column', 'of', 'words', 'cannot', 'adequately', 'describe', 'co', '-', 'writer', '/', 'director', 'peter', 'jackson', \"'\", 's', 'expanded', 'vision', 'of', 'j', '.', 'r', '.', 'r', '.', 'tolkien', \"'\", 's', 'middle', '-', 'earth', '.', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"][1])\n",
    "print(tokens)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0f6fd4-b80b-466d-b4ff-9aac0492c062",
   "metadata": {},
   "source": [
    "#### Exercise 1.3: A Stable Baseline\n",
    "\n",
    "In this exercise I want you to:\n",
    "1. Use Distilbert as a *feature extractor* to extract representations of the text strings from the dataset splits;\n",
    "2. Train a classifier (your choice, by an SVM from Scikit-learn is an easy choice).\n",
    "3. Evaluate performance on the validation and test splits.\n",
    "\n",
    "These results are our *stable baseline* -- the **starting** point on which we will (hopefully) improve in the next exercise.\n",
    "\n",
    "**Hint**: There are a number of ways to implement the feature extractor, but probably the best is to use a [feature extraction `pipeline`](https://huggingface.co/tasks/feature-extraction). You will need to interpret the output of the pipeline and extract only the `[CLS]` token from the *last* transformer layer. *How can you figure out which output that is?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5943bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "L'estrattore delle features costruito cosi restituisce una lista lunga quanto i campioni \n",
    "Le quali contengono al suo interno una lista composta da le liste di token della frase\n",
    "Ora per come viene usato il tokenizzatore in questione il primo vettore risulta essere quello di CLS mentre i successivi sono quelli ovviameente della parola della frase\n",
    "infine come ultimi può inserire dei token di padding oppure di separazione.\n",
    "'''\n",
    "def extract_features_with_pipeline(model, tokenizer, texts, batch_size=32):\n",
    "    #costruisco extractor\n",
    "    extractor = pipeline(\n",
    "        \"feature-extraction\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        framework=\"pt\"\n",
    "    )\n",
    "\n",
    "    all_features = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc= \"Features Extraction\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        #passo il batch di testi \n",
    "        outputs = extractor(batch_texts)\n",
    "\n",
    "        # estrai CLS per ogni frase che è nella posizione [0][0]\n",
    "        cls_batch = [sentence[0][0] for sentence in outputs]\n",
    "        all_features.extend(cls_batch)\n",
    "\n",
    "    return np.array(all_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7965af86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FitSVM(model, clf, tokenizer, dataset):\n",
    "    features= extract_features_with_pipeline(model, tokenizer,dataset[\"text\"])\n",
    "    labels= np.array(dataset[\"label\"])\n",
    "    clf.fit(features, labels)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b8be625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_report(dir,acc=None, cm=None, report=None,name_model=\"svm\",validation=True):\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    labels = [\"negative\", \"positive\"]\n",
    "    report_dict = {\n",
    "        \"accuracy\": acc if acc is not None else None,\n",
    "        \"classification_report\": report if report is not None else None,\n",
    "        \"confusion_matrix\": {labels[i]: {labels[j]: int(cm[i,j]) for j in range(len(labels))} for i in range(len(labels))} if cm is not None else None\n",
    "    }\n",
    "    if validation:\n",
    "        path= f\"{dir}/{name_model}_report_validation.json\"\n",
    "    else:\n",
    "        path=f\"{dir}/{name_model}_report_test.json\"\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(report_dict, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5146e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateSVM(model, clf, tokenizer, datasets,dir,name_model=\"svm\",validation=True):\n",
    "    features_validation= extract_features_with_pipeline(model, tokenizer, datasets[\"text\"])\n",
    "    labels_validation= np.array(valid_dataset[\"label\"])\n",
    "    y_pred = clf.predict(features_validation)\n",
    "    acc = accuracy_score(labels_validation, y_pred)\n",
    "    report = classification_report(labels_validation, y_pred, target_names=[\"negative\", \"positive\"], output_dict=True)\n",
    "    acc = accuracy_score(labels_validation, y_pred)\n",
    "    cm = confusion_matrix(labels_validation, y_pred)\n",
    "    save_report(dir, acc, cm ,report,name_model, validation)\n",
    "    return report, acc, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebb961ab-aa24-4d5c-86a1-c9b4237b40fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nPer come viene processato il token all' interno di bert si ha che le frasi vengono rappresentati con il token speciale\\n[CLS] come primo token che da una rappresentazione globale della frase, di norma usato proprio per la classificazione\\nInfatti questo tipo di token è ottimizzato per riassumere il significato intero dell' intera frase.\\nInfatti dall' esempio prima si vede come la riconversione in token diano la parola e come primo token sempre il CLS.\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Per come viene processato il token all' interno di bert si ha che le frasi vengono rappresentati con il token speciale\n",
    "[CLS] come primo token che da una rappresentazione globale della frase, di norma usato proprio per la classificazione\n",
    "Infatti questo tipo di token è ottimizzato per riassumere il significato intero dell' intera frase.\n",
    "Infatti dall' esempio prima si vede come la riconversione in token diano la parola e come primo token sempre il CLS.\n",
    "'''\n",
    "#seed= 42\n",
    "#max_iter_svm =500000\n",
    "#model = AutoModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "#clf = LinearSVC(max_iter=max_iter_svm)\n",
    "#\n",
    "#now = datetime.now()\n",
    "#formatted_data = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "#dir= f'run_{formatted_data}_{max_iter_svm}'\n",
    "#os.makedirs(dir, exist_ok=True)\n",
    "#clf= FitSVM(model, clf, tokenizer, train_dataset.shuffle(seed=seed))\n",
    "#_ = EvaluateSVM(model, clf, tokenizer, valid_dataset, dir)\n",
    "#_ = EvaluateSVM(model, clf, tokenizer, test_dataset, dir,\"svm\",False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37141d1b-935b-425c-804c-b9b487853791",
   "metadata": {},
   "source": [
    "-----\n",
    "### Exercise 2: Fine-tuning Distilbert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a53f64-0238-42f2-bc20-86e51c77d2e5",
   "metadata": {},
   "source": [
    "In this exercise we will fine-tune the Distilbert model to (hopefully) improve sentiment analysis performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3392b1ed-597b-4a92-90fc-10eb11eac515",
   "metadata": {},
   "source": [
    "#### Exercise 2.1: Token Preprocessing\n",
    "\n",
    "The first thing we need to do is *tokenize* our dataset splits. Our current datasets return a dictionary with *strings*, but we want *input token ids* (i.e. the output of the tokenizer). This is easy enough to do my hand, but the HugginFace `Dataset` class provides convenient, efficient, and *lazy* methods. See the documentation for [`Dataset.map`](https://huggingface.co/docs/datasets/v3.5.0/en/package_reference/main_classes#datasets.Dataset.map).\n",
    "\n",
    "**Tip**: Verify that your new datasets are returning for every element: `text`, `label`, `intput_ids`, and `attention_mask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e6e2a95-5b08-4f81-b824-59a0fb3404e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\", split=\"train\")\n",
    "valid_dataset = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\", split=\"validation\")\n",
    "test_dataset  = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2545d673",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba64ff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding= True,\n",
    "        truncation= True,\n",
    "        return_tensors=\"pt\" \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37b733bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train= train_dataset.map(tokenizer_fn, batched=True)\n",
    "tokenized_validation= valid_dataset.map(tokenizer_fn, batched=True)\n",
    "tokenized_test= test_dataset.map(tokenizer_fn, batched=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31ee5250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 8530\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1066\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1066\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train)\n",
    "print(tokenized_validation)\n",
    "print(tokenized_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a80de2-83c9-4c12-af4e-34babe23ffd1",
   "metadata": {},
   "source": [
    "#### Exercise 2.2: Setting up the Model to be Fine-tuned\n",
    "\n",
    "In this exercise we need to prepare the base Distilbert model for fine-tuning for a *sequence classification task*. This means, at the very least, appending a new, randomly-initialized classification head connected to the `[CLS]` token of the last transformer layer. Luckily, HuggingFace already provides an `AutoModel` for just this type of instantiation: [`AutoModelForSequenceClassification`](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodelforsequenceclassification). You will want you instantiate one of these for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6327d73-3b71-478a-9932-bb062a650c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "tokenizer= DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d109f7bd-955f-4fc4-bc3e-75bd99a17adf",
   "metadata": {},
   "source": [
    "#### Exercise 2.3: Fine-tuning Distilbert\n",
    "\n",
    "Finally. In this exercise you should use a HuggingFace [`Trainer`](https://huggingface.co/docs/transformers/main/en/trainer) to fine-tune your model on the Rotten Tomatoes training split. Setting up the trainer will involve (at least):\n",
    "\n",
    "\n",
    "1. Instantiating a [`DataCollatorWithPadding`](https://huggingface.co/docs/transformers/en/main_classes/data_collator) object which is what *actually* does your batch construction (by padding all sequences to the same length).\n",
    "2. Writing an *evaluation function* that will measure the classification accuracy. This function takes a single argument which is a tuple containing `(logits, labels)` which you should use to compute classification accuracy (and maybe other metrics like F1 score, precision, recall) and return a `dict` with these metrics.  \n",
    "3. Instantiating a [`TrainingArguments`](https://huggingface.co/docs/transformers/v4.51.1/en/main_classes/trainer#transformers.TrainingArguments) object using some reasonable defaults.\n",
    "4. Instantiating a `Trainer` object using your train and validation splits, you data collator, and function to compute performance metrics.\n",
    "5. Calling `trainer.train()`, waiting, waiting some more, and then calling `trainer.evaluate()` to see how it did.\n",
    "\n",
    "**Tip**: When prototyping this laboratory I discovered the HuggingFace [Evaluate library](https://huggingface.co/docs/evaluate/en/index) which provides evaluation metrics. However I found it to have insufferable layers of abstraction and getting actual metrics computed. I suggest just using the Scikit-learn metrics..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77fbd597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \n",
    "    logits, labels= eval_pred\n",
    "    if isinstance(logits, torch.Tensor):\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        labels= labels.detach().cpu().numpy()\n",
    "    #prendo la classe con probabilità maggiore\n",
    "    preds= np.argmax(logits, axis=-1)\n",
    "    \n",
    "    report = classification_report(labels, preds, output_dict=True)\n",
    "    \n",
    "    metrics={\n",
    "        \"accuracy\": report[\"accuracy\"],\n",
    "        \"precision\": report[\"macro avg\"][\"precision\"],\n",
    "        \"recall\": report[\"macro avg\"][\"recall\"],\n",
    "        \"f1-score\": report[\"macro avg\"][\"f1-score\"],\n",
    "        \n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "115f7305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trainer(model, data_collator=None,compute_metrics=None,output_dir=\"./\", \n",
    "                  train_dataset=None, eval_dataset=None, num_epochs=3,train_batch_size=8,\n",
    "                  eval_batch_size=8,lr=5e-5):\n",
    "    \n",
    "\n",
    "    trainer_args= transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=train_batch_size,\n",
    "        per_device_eval_batch_size= eval_batch_size,\n",
    "        learning_rate=lr,\n",
    "        num_train_epochs=num_epochs,\n",
    "        eval_strategy=\"epoch\",\n",
    "        remove_unused_columns=False\n",
    "    )\n",
    "    \n",
    "    trainer = transformers.Trainer(\n",
    "        model= model,\n",
    "        args=trainer_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33e91cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer= DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "#model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "#\n",
    "#\n",
    "#\n",
    "#now = datetime.now()\n",
    "#formatted_data = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "#general_dir= \"Trainer_Distilbert\"\n",
    "#dir= f'{general_dir}/run_{formatted_data}'\n",
    "#os.makedirs(dir, exist_ok=True)\n",
    "#data_collator= transformers.DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "#trainer= build_trainer(model, data_collator ,dir, compute_metrics, tokenized_train, tokenized_validation,3,16,16)\n",
    "#\n",
    "#trainer.train()\n",
    "#evaluation_metrics=trainer.evaluate()\n",
    "#\n",
    "#save_report(dir, report=evaluation_metrics, name_model=\"DistilBert\")\n",
    "#test_metric=trainer.evaluate(tokenized_test)\n",
    "#save_report(dir, report=test_metric, name_model=\"DistilBert\", validation=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8376de-8554-4a13-aac3-59257f3eb3fd",
   "metadata": {},
   "source": [
    "-----\n",
    "### Exercise 3: Choose at Least One\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b55cf4d-e64b-47fc-b8d5-37288b72d90d",
   "metadata": {},
   "source": [
    "#### Exercise 3.1: Efficient Fine-tuning for Sentiment Analysis (easy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f183856-1111-4fe9-81f1-691fe7c1b706",
   "metadata": {},
   "source": [
    "In Exercise 2 we fine-tuned the *entire* Distilbert model on Rotten Tomatoes. This is expensive, even for a small model. Find an *efficient* way to fine-tune Distilbert on the Rotten Tomatoes dataset (or some other dataset).\n",
    "\n",
    "**Hint**: You could check out the [HuggingFace PEFT library](https://huggingface.co/docs/peft/en/index) for some state-of-the-art approaches that should \"just work\". How else might you go about making fine-tuning more efficient without having to change your training pipeline from above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ea6bca5-9b36-424e-898c-52c0777eae5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeca737-ee00-4d98-a3ae-f4d6eb3d264f",
   "metadata": {},
   "source": [
    "#### Exercise 3.2: Fine-tuning a CLIP Model (harder)\n",
    "\n",
    "Use a (small) CLIP model like [`openai/clip-vit-base-patch16`](https://huggingface.co/openai/clip-vit-base-patch16) and evaluate its zero-shot performance on a small image classification dataset like ImageNette or TinyImageNet. Fine-tune (using a parameter-efficient method!) the CLIP model to see how much improvement you can squeeze out of it.\n",
    "\n",
    "**Note**: There are several ways to adapt the CLIP model; you could fine-tune the image encoder, the text encoder, or both. Or, you could experiment with prompt learning.\n",
    "\n",
    "**Tip**: CLIP probably already works very well on ImageNet and ImageNet-like images. For extra fun, look for an image classification dataset with different image types (e.g. *sketches*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24da83f5",
   "metadata": {},
   "source": [
    "CLIPmodel: Contrastive Language-image pretraining è un modello.\n",
    "\n",
    "È un modello sviluppato da OpenAI con l’idea di mettere immagini e testo nello stesso spazio semantico.\n",
    "In pratica:\n",
    "1) CLIP prende un’immagine e la trasforma in un vettore numerico (image embedding).\n",
    "2) CLIP prende una descrizione testuale e la trasforma in un vettore numerico (text embedding).\n",
    "3) Poi calcola quanto i due vettori sono simili (cosine similarity).\n",
    "\n",
    "L’addestramento è contrastive, cioè il modello impara a:\n",
    "- Avvicinare embeddings di immagini e testi che corrispondono (es. immagine di un gatto + testo “a photo of a cat”)\n",
    "- Allontanare embeddings di immagini e testi che non corrispondono (es. immagine di un cane + testo “a photo of a cat”)\n",
    "\n",
    "In questo metodo si utilizza due componenti:\n",
    "- CLIPModel: è il modello vero e proprio che trasforma immagini e testo in embedding\n",
    "- CLIPProcessor: serve invece a preparare i dati in modo tale da poterli preprocessare attraverso operazioni di:\n",
    "    - normalizzazione immagini\n",
    "    - ridimensionamento delle immagini\n",
    "    - tokenizzazione del testo\n",
    "\n",
    "Quindi il processor prepara i dati per come il modello se li aspetta secondo i valori di CLIP mentre il modello vero e proprio trasforma i dati in embedding e calcola la loro similarità con la distanza del coseno "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00b59ec2-4fe6-44c6-ab0b-0069f486bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "import peft\n",
    "from peft import  LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0b54462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f89c69a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Sijuade/ImageNette\")\n",
    "dataset_train= load_dataset(\"Sijuade/ImageNette\", split=\"train\")\n",
    "dataset_validation= load_dataset(\"Sijuade/ImageNette\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5c88b2",
   "metadata": {},
   "source": [
    "Il problema è che qua le immagini corrispondono a label e questo ovviamente porta a delle problematiche ecco perchè si parla di fine-tuning.\n",
    "In questo caso si parla di zero-shot quindi daremo impasto al modello un prompt molto smplice:\n",
    "come \" questa è una figura di: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "deafb792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label', 'text_prompt'],\n",
       "    num_rows: 9469\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names= dataset_train.features[\"label\"].names\n",
    "\n",
    "def label_to_prompt(batch):\n",
    "    readable_name = class_names[batch['label']].replace('_', ' ')\n",
    "    batch['text_prompt'] = f\"This is a photo of {readable_name}\"\n",
    "    return batch\n",
    "\n",
    "train_dataset_converted=dataset_train.map(label_to_prompt)\n",
    "validation_dataset_converted= dataset_validation.map(label_to_prompt)\n",
    "train_dataset_converted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6522277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#definisco funzione di collate per poter andare a costruire i batch \n",
    "candidate_labels = [f'This is a photo of {label}.' for label in class_names]\n",
    "def collate_fn(batch):\n",
    "    images = [x['image'] for x in batch]\n",
    "    labels = torch.tensor([x['label'] for x in batch])\n",
    "    inputs = processor(text=candidate_labels, images=images, return_tensors=\"pt\", padding=True)\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c55a8b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Validation_zero_shot():\n",
    "    device= \"cuda:0\"\n",
    "    dataloader_validation= DataLoader(validation_dataset_converted,batch_size=512, collate_fn=collate_fn)\n",
    "    all_preds=[]\n",
    "    all_labels=[]\n",
    "    model.to(device)\n",
    "    for batch in tqdm(dataloader_validation, desc=\"calculate the validation zero-shot\"):\n",
    "        inputs, label= batch\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}  # se inputs è un dict di tensor\n",
    "        label = label.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        logits = outputs.logits_per_image  # shape: [batch_size, num_classes]\n",
    "        probs = logits.softmax(dim=-1)     # softmax sul device\n",
    "\n",
    "        # se vuoi usare numpy per calcolare argmax:\n",
    "        probs_np = probs.cpu().numpy()\n",
    "        pred_idx = probs_np.argmax(axis=1)  # argmax per ogni immagine della batch\n",
    "        all_preds.extend(pred_idx)\n",
    "        all_labels.extend(label.cpu().numpy())\n",
    "        \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    report= classification_report(all_labels, all_preds,  target_names=class_names)\n",
    "    print(report)\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535f46ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_zero_shot= Validation_zero_shot()\n",
    "dir= \"Exercise3/Validation_Zero_Shot\"\n",
    "save_report(dir,report=repo_zero_shot,name_model=\"ZeroShot_OpenAI\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa59a0d7",
   "metadata": {},
   "source": [
    "Vediamo come il modello comunque riesce ad avere un ottima acuratezza quindi vediamo di provare anche a fine tunare se riusciamo a ricavare qualcosa di migliore\n",
    "Poi cosi proveremo anche a cambiare dataset\n",
    "\n",
    "vediamo che per il fine tuning viene proposto di Utilizzare LORA per poter fine-tunare diverse parti del modello, dal prompt al visual encoder al text encoder.\n",
    "Vediamo dunque di partire:\n",
    "- text encoding\n",
    "- visual encoding\n",
    "- entrambi\n",
    "LoRA (Low-Rank Adaptation) è un metodo di fine-tuning efficiente che aggiunge piccole matrici a basso rango alle proiezioni dei Transformer, permettendo di adattare il modello con pochissimi parametri, lasciando il backbone congelato.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1fbc1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Lora_Config(model, text_encoder=True, visual_encoder=False):\n",
    "    config = LoraConfig(\n",
    "        r=4,\n",
    "        lora_alpha=14,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        bias=\"none\"\n",
    "    )\n",
    "    if text_encoder:\n",
    "        model.text_model= get_peft_model( model.text_model, config)\n",
    "    if visual_encoder:\n",
    "        model.vision_model= get_peft_model( model.vision_model, config)\n",
    "    print(f\"Text model params:\")\n",
    "    model.text_model.print_trainable_parameters()\n",
    "    if visual_encoder:\n",
    "        print(f\"Vision model params:\")\n",
    "        model.vision_model.print_trainable_parameters()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9284c724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_report(all_labels, all_preds, class_names):\n",
    "    report = classification_report(\n",
    "        all_labels,\n",
    "        all_preds,\n",
    "        target_names=class_names,\n",
    "        output_dict=True,\n",
    "        digits=4\n",
    "    )\n",
    "    print(report)\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0f267cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text model params:\n",
      "trainable params: 98,304 || all params: 63,264,256 || trainable%: 0.1554\n"
     ]
    }
   ],
   "source": [
    "model_lora=build_Lora_Config(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2af4c35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model,\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    device\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs, labels = batch\n",
    "\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits_per_image   # [B, C]\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a661bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model,\n",
    "    dataloader,\n",
    "    device\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluation\"):\n",
    "            inputs, labels = batch\n",
    "\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits_per_image\n",
    "            preds = logits.argmax(dim=1)\n",
    "\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    accuracy = (all_preds == all_labels).mean()\n",
    "\n",
    "    return accuracy, all_preds, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d08fa16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/592 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 592/592 [06:24<00:00,  1.54it/s]\n",
      "Evaluation: 100%|██████████| 592/592 [03:05<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0678\n",
      "Train acc : 0.9808\n",
      "Val acc   : 0.9695\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 98/592 [01:04<05:23,  1.53it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m train_one_epoch(\n\u001b[1;32m     17\u001b[0m         model_lora,\n\u001b[1;32m     18\u001b[0m         train_dataloader,\n\u001b[1;32m     19\u001b[0m         optimizer,\n\u001b[1;32m     20\u001b[0m         device\n\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     23\u001b[0m     val_acc, val_preds, val_labels \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[1;32m     24\u001b[0m         model_lora,\n\u001b[1;32m     25\u001b[0m         val_dataloader,\n\u001b[1;32m     26\u001b[0m         device\n\u001b[1;32m     27\u001b[0m     )\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[35], line 28\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, device)\u001b[0m\n\u001b[1;32m     25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 28\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     30\u001b[0m preds \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     31\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_lora.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-5,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "num_epochs = 5\n",
    "train_dataloader= DataLoader(train_dataset_converted,batch_size=16, collate_fn=collate_fn, shuffle=True)\n",
    "val_dataloader=  DataLoader(validation_dataset_converted,batch_size=16, collate_fn=collate_fn)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model_lora,\n",
    "        train_dataloader,\n",
    "        optimizer,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    val_acc, val_preds, val_labels = evaluate(\n",
    "        model_lora,\n",
    "        val_dataloader,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    print(f\"Train loss: {train_loss:.4f}\")\n",
    "    print(f\"Train acc : {train_acc:.4f}\")\n",
    "    print(f\"Val acc   : {val_acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe42ed48-444d-47d4-bd8b-839a99e7996a",
   "metadata": {},
   "source": [
    "#### Exercise 3.3: Choose your Own Adventure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196f9129-ef2e-45f7-9e8f-baa697ccd91e",
   "metadata": {},
   "source": [
    "There are a *ton* of interesting and fun models on the HuggingFace hub. Pick one that does something interesting and adapt it in some way to a new task. Or, combine two or more models into something more interesting or fun. The sky's the limit.\n",
    "\n",
    "**Note**: Reach out to me by email or on the Discord if you are unsure about anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c150bd36-6535-4724-a06d-a61632d3132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
